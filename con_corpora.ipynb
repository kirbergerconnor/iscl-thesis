{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, urllib, unicodedata, os.path, glob, ebooklib\n",
    "from ebooklib import epub\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lingua France Nova\n",
    "\n",
    "lfn_filtered = []\n",
    "\n",
    "with open('./Languages/lfn/Corpus/corpo.txt', 'r', encoding='utf-8') as lfn_f:\n",
    "    text = lfn_f.readlines()\n",
    "    lfn_f.close()\n",
    "sentences = [sentence for sentence in text if not sentence.startswith('@') and not sentence.startswith('=') and not sentence[0].isdigit()]\n",
    "alpha = re.compile(r'[^a-z\\s]+')\n",
    "lfn_filtered += [' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence]\n",
    "    \n",
    "with open('Lfn.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(lfn_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interlingua\n",
    "\n",
    "ia_filtered = []\n",
    "\n",
    "with open('./Languages/interlingua/English_and_Interlingua_Parallel_Sentences.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().splitlines()\n",
    "    f.close()\n",
    "sentences = [line.lower().strip('[ina]') for line in text if '[ENG]' not in line]\n",
    "for line in text:\n",
    "    line = re.sub(r'\\d+', '', line)\n",
    "    line = re.sub(r'[^a-z\\s]', '', line)\n",
    "    line = ' '.join([word.strip() for word in line.split()])\n",
    "    if len(line) > 0:\n",
    "        ia_filtered.append(line.strip())\n",
    "            \n",
    "with open('./Languages/interlingua/InterlinguaSentences.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().splitlines()\n",
    "    f.close()\n",
    "text = [line.lower() for line in text]\n",
    "for line in text:\n",
    "    line = re.sub(r'\\d+', '', line)\n",
    "    line = re.sub(r'[^a-z\\s]', '', line)\n",
    "    line = ' '.join([word.strip() for word in line.split()])\n",
    "    if len(line) > 0:\n",
    "        ia_filtered.append(line.strip())\n",
    "\n",
    "with open('./Languages/interlingua/ina_wikipedia_2021_30K-sentences.txt') as f:\n",
    "    text = f.read().splitlines()\n",
    "    f.close()\n",
    "text = [s.split('\\t')[1].lower() for s in text]\n",
    "for line in text:\n",
    "    line = re.sub(r'\\d+', '', line)\n",
    "    line = re.sub(r'[^a-z\\s]', '', line)\n",
    "    line = ' '.join([word.strip() for word in line.split()])\n",
    "    if len(line) > 0:\n",
    "        ia_filtered.append(line.strip())\n",
    "\n",
    "with open('Interlingua.txt', 'w', encoding='utf-8') as f:\n",
    "    ia_filtered = list(set(ia_filtered))\n",
    "    f.write('\\n'.join(ia_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperanto\n",
    "\n",
    "eo_filtered = []\n",
    "\n",
    "with open('./Languages/esperanto/epo_wikipedia_2021_300K-sentences.txt', 'r', encoding='utf-8') as eo_f:\n",
    "    text = eo_f.readlines()\n",
    "    eo_f.close()\n",
    "sentences = [line.lower() for line in text]\n",
    "alpha = re.compile(r'[^\\u0061-\\u0070\\u0072-\\u0076\\u007A\\u0109\\u011D\\u0125\\u0135\\u015D\\u016D\\s]+')\n",
    "eo_filtered += [' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence]\n",
    "            \n",
    "with open('Esperanto.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(eo_filtered))\n",
    "    f.close()\n",
    "\n",
    "# eo_sentences = []\n",
    "# for eo_file in glob.glob(os.path.join('./esperanto_corpus', '*.html')):\n",
    "#     with open(eo_file) as eo_html_f:\n",
    "#         eo_soup = BeautifulSoup(eo_html_f)\n",
    "#         remove = eo_soup.find(\"div\", attrs={\"class\": \"tekstokapo\"})\n",
    "#         remove.extract()\n",
    "#         eo_res = [unicodedata.normalize('NFKC', tag.get_text().lower()) for tag in eo_soup.find(\"div\", attrs={\"class\": \"tekstarteksto\"}).find_all('p')]\n",
    "#         eo_res = [line for line in eo_res if not line.startswith('*')]\n",
    "#         eo_sentences += eo_res\n",
    "#         eo_html_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lojban\n",
    "\n",
    "jbo_filtered = []\n",
    "\n",
    "with open('./Languages/lojban/Corpus/corpus.txt', 'r', encoding='utf-8') as jbo_f:\n",
    "    text = jbo_f.readlines()\n",
    "    jbo_f.close()\n",
    "sentences = [line.lower() for line in text]\n",
    "alpha = re.compile(r'[^\\u0061-\\u0067\\u0070\\u0072-\\u0076\\u0078-\\u007A\\u0027\\u002C\\u002E\\s]+')\n",
    "jbo_filtered.extend([' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence])\n",
    "\n",
    "# with open('./Languages/lojban/Corpus/corpuscopy.txt', encoding='UTF-8') as file_in:\n",
    "#     with open('./Languages/lojban/Corpus/filtered_corpus.txt', 'w', encoding='UTF-8') as file_out:\n",
    "#         for line in file_in:\n",
    "#             line = re.sub(r'^.*<.*>', '', line, flags=re.MULTILINE)\n",
    "#             line = re.sub(r'\\[.*\\] <.*>', '', line)\n",
    "#             line = re.sub(r'-header(.|\\s)*?start-+', '', line)\n",
    "#             line = re.sub(r'-end-+', '', line)\n",
    "#             line = re.sub(r'(http|https)://([\\w-]+\\.)+[\\w-]+(/[\\w\\-\\./\\?%&=]*)?', '', line)\n",
    "#             line = re.sub(r'[^A-Za-z\\'\\.\\, ]', '', line)\n",
    "#             line = re.sub(r'\\w*h\\w*', '', line)\n",
    "#             line = re.sub(r'(^|\\W)[A-Z][a-z]+', '', line)\n",
    "#             line = line.lower()\n",
    "#             file_out.write(line + '\\n')\n",
    "        \n",
    "with open('Lojban.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(jbo_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quenya\n",
    "\n",
    "with open('quenyabible.rtf', 'w', encoding='utf-8') as output:\n",
    "    for file in glob.glob('./Languages/quenya/*.rtf'):\n",
    "        f = open(file)\n",
    "        for line in f:\n",
    "            if re.search(r'\\\\f2.*|\\\\f3.*', line):\n",
    "                continue\n",
    "            output.write(line)\n",
    "        \n",
    "# doc = open('quenyabible.rtf', 'r', encoding='utf-8')\n",
    "# text = doc.read()\n",
    "# x = rtf_to_text(text, encoding='utf-8')\n",
    "# with open('quenyabible.txt', 'w') as out:\n",
    "#     out.write(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lingwa de Planeta\n",
    "\n",
    "# # Humanist Speech PDF\n",
    "# ldp_outstr = StringIO()\n",
    "# with open('./Languages/ldp/humanist_speech_trans.pdf', 'rb') as ldp_f_1:\n",
    "#     extract_text_to_fp(ldp_f_1, ldp_outstr, laparams=LAParams(),\n",
    "#                        output_type='html', codec=None)\n",
    "    \n",
    "# ldp_soup_1 = BeautifulSoup(ldp_outstr.getvalue())\n",
    "# ldp_text_1 = [sen.get_text().strip().replace(\"\\n\", \" \").lower() for sen in ldp_soup_1.find_all(\"span\", style=\"font-family: font00000000296ca43f; font-size:12px\") if \"\\nPage\" not in sen.get_text()]\n",
    "\n",
    "\n",
    "# # Animal Farm PDF\n",
    "# ldp_outstr_2 = StringIO()\n",
    "# with open('./Languages/ldp/animal_farm_trans.pdf', 'rb') as ldp_f_2:\n",
    "#     extract_text_to_fp(ldp_f_2, ldp_outstr_2, laparams=LAParams(),\n",
    "#                        output_type='html', codec=None)\n",
    "\n",
    "# ldp_soup_2 = BeautifulSoup(ldp_outstr_2.getvalue())\n",
    "# ldp_text_2 = [sen.get_text().strip().replace(\"\\n\", \" \").lower() for sen in ldp_soup_2.find_all([\"spa\"], style=\"font-family: font00000000296cac0e; font-size:12px\") if \"\\nPage\" not in sen.get_text()]\n",
    "\n",
    "# with open('LdP.txt', 'w', encoding='utf-8') as ldp_out_f, open('conlangs_corpus.txt', 'w', encoding='utf-8') as corpus:\n",
    "#     ldp_out_f.write(\" \".join(ldp_text_1))\n",
    "#     ldp_out_f.write(\" \".join(ldp_text_2))\n",
    "#     ldp_out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Qo’noS ta’puq,\\r\\n  Hamlet lotlut.', 'LUTVAD GHOTVAM LUDALU’', 'TLHAW’DIYUS, Qo’noS ta’ ghaH\\r\\n  HAMLET, ben ta’ puqloD; DaHjaj ta’ loDnI’puqloD je ghaH\\nPOLONYUS, Qang ghaH\\r\\n  HOREY’SO, Hamlet jup ghaH\\nLAYERTES, polonyuS puqloD ghaH\\nVOLTIMAD\\nQORNELYUS\\nROSENQATLH\\nGHILDESTEN\\r\\n  ’OSRIQ\\r\\n  wa’ ’utlh\\r\\n  wa’ lalDanyaS\\nMARSE’LUS\\nBERNARDO\\nVERANCHISQO, mang ghaH\\nREYNALDO, polonyuS toy’wI’ ghaH\\r\\n  vagh DawI’pu’\\r\\n  cha’ tlhaQwI’; molwI’ Da\\nVORTIBRAS, DuraS tuq pIn be’nI’puqloD ghaH\\r\\n  wa’ HoD\\r\\n  tera’ Duypu’\\r\\n  wa’ yaS\\r\\n  wa’ mang\\r\\n  cha’ QumwI’\\nHorey’So wa’ toy’wI’\\r\\n  QonoSnganpu’\\nHamlet vav lomqa’', 'GHERTLHUD, Qo’noS ta’be’, Hamlet SoS je ghaH\\r\\n  ’OVELYA, polonyuS puqbe’ ghaH\\r\\n  jawloDpu’, jawbe’pu’, yaSpu’, mangpu’, yo’mangpu’, latlh toy’wI’pu’ je; jatlhbe’ chaH\\nLUT DAQ: Qo’noS', 'LUT ’AY’ WA’\\nLUT ’AY’HOM WA’ tlhIn. ta’qach’a’ tlhop ’avwI’Daq jen.\\r\\n  [’avtaH VERANCHISQO. ghaHDaq ’el BERNARDO]', 'bernarDo', 'chol ’Iv?', 'veranchISqo', 'Qo’, jIH HIjang. yItaDchoH ’ej yIngu’egh.', 'bernarDo', 'taHjaj wo’!', 'veranchISqo', 'bernarDo?', 'bernarDo', 'jIHbej.', 'veranchISqo', 'bImatlhba’, qarqu’mo’ bIcholmeH poHlIj.', 'bernarDo', 'qaSpu’ ramjep. QongDaq yIghoS, veranchISqo.', 'veranchISqo']\n"
     ]
    }
   ],
   "source": [
    "# # Klingon\n",
    "\n",
    "# klingon_content = urllib.request.urlopen('http://klingon.wiki/En/TheKlingonWayPhrases')\n",
    "# read_content = klingon_content.read()\n",
    "# klingon_soup = BeautifulSoup(read_content, 'html.parser')\n",
    "\n",
    "# with open('Klingon.txt', 'w', encoding='utf-8') as klingon_f:\n",
    "#     for sen in klingon_soup.find_all(\"b\", limit=156):\n",
    "#         klingon_f.write(sen.get_text() + \" \")\n",
    "\n",
    "# book = epub.read_epub('/Users/k/Downloads/TheKlingonLanguageVersionOfTheWorl-Anderson_Joeltranslator.epub')\n",
    "# contents = []\n",
    "# for item in book.get_items():\n",
    "#     if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "#         contents.append(item.get_content())\n",
    "\n",
    "# with open('klingonbible.html', 'w', encoding='utf-8') as f:\n",
    "#     f.write('<html>\\n<head>\\n<meta charset=\"utf-8\">\\n</head>\\n<body>\\n')\n",
    "#     for content in contents:\n",
    "#         f.write('<div>\\n')\n",
    "#         f.write(content.decode('utf-8'))\n",
    "#         f.write('\\n</div>\\n')\n",
    "#     f.write('</body>\\n</html>')\n",
    "\n",
    "# with open('./Languages/klingon/klingonbible.html') as f:\n",
    "#     content = f.read()\n",
    "#     f.close()\n",
    "# soup = BeautifulSoup(content, 'html.parser')\n",
    "# exclude = ['previous next', 'WEB ']\n",
    "# text = [s.get_text().strip('\\n') for s in soup.find_all('p') if s.get_text().strip('\\n') and s.get_text().strip('\\n') not in exclude]\n",
    "# with open('./Languages/klingon/klingonbible.txt', 'w', encoding='utf-8') as f:\n",
    "#     for line in text:\n",
    "#         line = re.sub(r'\\d+:\\d', '', line)\n",
    "#         line = re.sub(r'\\s+', ' ', line)\n",
    "#         line = re.sub(r'\\\\t', ' ', line)\n",
    "#         line = line.strip()\n",
    "#         f.write(line + '\\n')\n",
    "\n",
    "# book = epub.read_epub('/Users/k/Downloads/Strader, Andrew - The Klingon Hamlet (Pocket Books_Star Trek) - libgen.li.epub')\n",
    "# contents = []\n",
    "# for item in book.get_items():\n",
    "#     if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "#         contents.append(item.get_content())\n",
    "\n",
    "# with open('klingonhamlet.html', 'w', encoding='utf-8') as f:\n",
    "#     f.write('<html>\\n<head>\\n<meta charset=\"utf-8\">\\n</head>\\n<body>\\n')\n",
    "#     for content in contents:\n",
    "#         f.write('<div>\\n')\n",
    "#         f.write(content.decode('utf-8'))\n",
    "#         f.write('\\n</div>\\n')\n",
    "#     f.write('</body>\\n</html>')\n",
    "\n",
    "\n",
    "with open('./Languages/klingon/klingonhamlet.html') as f:\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "text = [s.get_text() for s in soup.find_all('p') if s.get_text()]\n",
    "with open('./Languages/klingon/klingonhamlet.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in text:\n",
    "        line = re.sub(r'\\d+:\\d', '', line)\n",
    "        line = re.sub(r'\\s+', ' ', line)\n",
    "        line = re.sub(r'\\\\t', ' ', line)\n",
    "        line = line.strip()\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dothraki\n",
    "\n",
    "# dothraki_outstr = StringIO()\n",
    "# with open('./Languages/dothraki/Dothraki_dic.pdf', 'rb') as dothraki_f:\n",
    "#     extract_text_to_fp(dothraki_f, dothraki_outstr, laparams=LAParams(),\n",
    "#                        output_type='html', codec=None)\n",
    "    \n",
    "# dothraki_soup = BeautifulSoup(dothraki_outstr.getvalue())\n",
    "# dothraki_res = [word.get_text() for word in dothraki_soup.find_all([\"span\"], style=\"font-family: URWPalladioL-Bold; font-size:10px\", limit=1455) if \"\\n\" not in word.get_text()] \n",
    "\n",
    "# with open('Dothraki.txt', 'w', encoding='utf-8') as dothraki_out_f, open('conlangs_corpus.txt', 'w', encoding='utf-8') as corpus:\n",
    "#     dothraki_out_f.write(\" \".join(dothraki_res))\n",
    "#     dothraki_out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
