{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, urllib, unicodedata, os.path, glob\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lingua France Nova\n",
    "\n",
    "lfn_filtered = []\n",
    "\n",
    "with open('./lfn/Corpus/corpo.txt', 'r', encoding='utf-8') as lfn_f:\n",
    "    text = lfn_f.readlines()\n",
    "    lfn_f.close()\n",
    "sentences = [sentence for sentence in text if not sentence.startswith('@') and not sentence.startswith('=') and not sentence[0].isdigit()]\n",
    "# alpha = r'[^a-z]'\n",
    "alpha = 'abcdefghijklmnopqrstuvwxyz'\n",
    "for sentence in sentences:\n",
    "    # s = ' '.join([re.sub(alpha, '', word.strip().lower()) for word in sentence.split()])\n",
    "    s = ' '.join([word.lower() for word in sentence.split() if all(letter in alpha for letter in word.lower())])\n",
    "    if len(s) > 0:\n",
    "            lfn_filtered.append(s)\n",
    "    \n",
    "with open('Lfn.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(lfn_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interlingua\n",
    "\n",
    "ia_filtered = []\n",
    "\n",
    "with open('./interlingua/English_and_Interlingua_Parallel_Sentences.txt', 'r', encoding='utf-8') as ia_f:\n",
    "    text = ia_f.readlines()\n",
    "    ia_f.close()\n",
    "    sentences = [line.lower().strip('[ina]').strip() for line in text if '[ENG]' not in line]\n",
    "    alpha = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    for sentence in sentences:\n",
    "        s = ' '.join([word.lower() for word in sentence.split() if all(letter in alpha for letter in word.lower())])\n",
    "        if len(s) > 0:\n",
    "            ia_filtered.append(s)\n",
    "\n",
    "with open('Interlingua.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(ia_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperanto\n",
    "\n",
    "eo_filtered = []\n",
    "\n",
    "with open('./esperanto/epo_wikipedia_2021_300K-sentences.txt', 'r', encoding='utf-8') as eo_f:\n",
    "    text = eo_f.read()\n",
    "    eo_f.close()\n",
    "sentences = re.split(r'[.!?]', unicodedata.normalize('NFKC', text))\n",
    "alpha = 'abcĉdefgĝhĥijĵklmnoprsŝtuŭvz'\n",
    "for sentence in sentences:\n",
    "    s = ' '.join([word.lower() for word in sentence.split() if all(letter in alpha for letter in word.lower())])\n",
    "    if len(s) > 0:\n",
    "        eo_filtered.append(s)\n",
    "            \n",
    "with open('Esperanto.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(eo_filtered))\n",
    "    f.close()\n",
    "\n",
    "# eo_sentences = []\n",
    "\n",
    "# for eo_file in glob.glob(os.path.join('./esperanto_corpus', '*.html')):\n",
    "#     with open(eo_file) as eo_html_f:\n",
    "#         eo_soup = BeautifulSoup(eo_html_f)\n",
    "#         remove = eo_soup.find(\"div\", attrs={\"class\": \"tekstokapo\"})\n",
    "#         remove.extract()\n",
    "#         eo_res = [unicodedata.normalize('NFKC', tag.get_text().lower()) for tag in eo_soup.find(\"div\", attrs={\"class\": \"tekstarteksto\"}).find_all('p')]\n",
    "#         eo_res = [line for line in eo_res if not line.startswith('*')]\n",
    "#         eo_sentences += eo_res\n",
    "#         eo_html_f.close()\n",
    "        \n",
    "# eo_output = eo_sentences\n",
    "\n",
    "# with open('Esperanto.txt', 'w', encoding='utf-8') as eo_out_f, open('conlangs_corpus.txt', 'w', encoding='utf-8') as corpus:\n",
    "#     eo_out_f.writelines(\"\\n\".join(eo_output))\n",
    "#     corpus.writelines(\"\\n\".join(eo_output))\n",
    "#     eo_out_f.close()\n",
    "#     corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lojban\n",
    "\n",
    "jbo_filtered = []\n",
    "\n",
    "with open('./lojban/Corpus/jb2en.tsv', 'r', encoding='utf-8') as jbo_f:\n",
    "    text = jbo_f.readlines()\n",
    "    jbo_f.close()\n",
    "sentences = [line.split('\\t')[0].strip() for line in text]\n",
    "alpha = 'abcdefgijklmnoprstuvxyz\\'\\.\\,'\n",
    "for sentence in sentences:\n",
    "    s = ' '.join([word.lower() for word in sentence.split() if all(letter in alpha for letter in word.lower())])\n",
    "    if len(s) > 0:\n",
    "        jbo_filtered.append(s)\n",
    "        \n",
    "with open('Lojban.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(jbo_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lingwa de Planeta\n",
    "\n",
    "# Humanist Speech PDF\n",
    "ldp_outstr = StringIO()\n",
    "with open('humanist_speech_trans.pdf', 'rb') as ldp_f_1:\n",
    "    extract_text_to_fp(ldp_f_1, ldp_outstr, laparams=LAParams(),\n",
    "                       output_type='html', codec=None)\n",
    "    \n",
    "ldp_soup_1 = BeautifulSoup(ldp_outstr.getvalue())\n",
    "ldp_text_1 = [sen.get_text().strip().replace(\"\\n\", \" \").lower() for sen in ldp_soup_1.find_all(\"span\", style=\"font-family: font00000000296ca43f; font-size:12px\") if \"\\nPage\" not in sen.get_text()]\n",
    "\n",
    "\n",
    "# Animal Farm PDF\n",
    "ldp_outstr_2 = StringIO()\n",
    "with open('animal_farm_trans.pdf', 'rb') as ldp_f_2:\n",
    "    extract_text_to_fp(ldp_f_2, ldp_outstr_2, laparams=LAParams(),\n",
    "                       output_type='html', codec=None)\n",
    "\n",
    "ldp_soup_2 = BeautifulSoup(ldp_outstr_2.getvalue())\n",
    "ldp_text_2 = [sen.get_text().strip().replace(\"\\n\", \" \").lower() for sen in ldp_soup_2.find_all([\"spa\"], style=\"font-family: font00000000296cac0e; font-size:12px\") if \"\\nPage\" not in sen.get_text()]\n",
    "\n",
    "with open('LdP.txt', 'w', encoding='utf-8') as ldp_out_f, open('conlangs_corpus.txt', 'w', encoding='utf-8') as corpus:\n",
    "    ldp_out_f.write(\" \".join(ldp_text_1))\n",
    "    ldp_out_f.write(\" \".join(ldp_text_2))\n",
    "    ldp_out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klingon\n",
    "\n",
    "klingon_content = urllib.request.urlopen('http://klingon.wiki/En/TheKlingonWayPhrases')\n",
    "read_content = klingon_content.read()\n",
    "klingon_soup = BeautifulSoup(read_content, 'html.parser')\n",
    "\n",
    "with open('Klingon.txt', 'w', encoding='utf-8') as klingon_f:\n",
    "    for sen in klingon_soup.find_all(\"b\", limit=156):\n",
    "        klingon_f.write(sen.get_text() + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dothraki\n",
    "\n",
    "dothraki_outstr = StringIO()\n",
    "with open('Dothraki_dic.pdf', 'rb') as dothraki_f:\n",
    "    extract_text_to_fp(dothraki_f, dothraki_outstr, laparams=LAParams(),\n",
    "                       output_type='html', codec=None)\n",
    "    \n",
    "dothraki_soup = BeautifulSoup(dothraki_outstr.getvalue())\n",
    "dothraki_res = [word.get_text() for word in dothraki_soup.find_all([\"span\"], style=\"font-family: URWPalladioL-Bold; font-size:10px\", limit=1455) if \"\\n\" not in word.get_text()] \n",
    "\n",
    "with open('Dothraki.txt', 'w', encoding='utf-8') as dothraki_out_f, open('conlangs_corpus.txt', 'w', encoding='utf-8') as corpus:\n",
    "    dothraki_out_f.write(\" \".join(dothraki_res))\n",
    "    dothraki_out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
