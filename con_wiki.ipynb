{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, glob, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperanto\n",
    "\n",
    "eo_filtered = []\n",
    "\n",
    "for file in glob.glob('./esperanto/Wiki/*.txt'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    sentences = re.split(r'[.!?]', unicodedata.normalize('NFKC', text))\n",
    "    alpha = 'abcĉdefgĝhĥijĵklmnoprsŝtuŭvz'\n",
    "    for sentence in sentences:\n",
    "        s = ' '.join([word.lower() for word in sentence.split() if all(letter in alpha for letter in word.lower())])\n",
    "        if len(s) > 0:\n",
    "            eo_filtered.append(s)\n",
    "\n",
    "eo_out.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interlingua\n",
    "\n",
    "ia_filtered = []\n",
    "\n",
    "for file in glob.glob('./interlingua/Wiki/*.txt'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    sentences = re.split(r'[.!?]\\s', unicodedata.normalize('NFKC', text))\n",
    "    alpha = r'[^a-z]'\n",
    "    for sentence in sentences:\n",
    "        s = ' '.join([re.sub(alpha, '', word.strip().lower()) for word in sentence.split()])\n",
    "        if len(s) > 0:\n",
    "            ia_filtered.append(s)\n",
    "\n",
    "ia_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lingua Franca Nova\n",
    "\n",
    "lfn_filtered = []\n",
    "alpha = r'[^a-z]'\n",
    "\n",
    "for file in glob.glob('./Languages/lfn/Wiki/*.txt'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    sentences = re.split(r'[.!?]\\s', text)\n",
    "    alpha = r'[^a-z]'\n",
    "    for sentence in sentences:\n",
    "        s = ' '.join([re.sub(alpha, '', word.strip().lower()) for word in sentence.split()])\n",
    "        if len(s) > 0:\n",
    "            lfn_filtered.append(s)\n",
    "            \n",
    "with open('./Languages/lfn/Corpus/corpo.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.readlines()\n",
    "    f.close()\n",
    "sentences = [s for s in text if not s.startswith('@') and not s.startswith('=') and not re.match(r'\\d{1,2}\\s.+\\d{4}', s)]\n",
    "for sentence in sentences:\n",
    "    s = ' '.join([re.sub(alpha, '', word.strip().lower()) for word in sentence.split()])\n",
    "    if len(s) > 0:\n",
    "        lfn_filtered.append(s.strip())\n",
    "\n",
    "with open('Lfn.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(lfn_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lojban\n",
    "\n",
    "jbo_filtered = []\n",
    "\n",
    "alpha = r'[^abcdefgijklmnoprstuvxyz\\'\\.\\,]'\n",
    "sep = [\".i\", \"ni'o\", \"ja'o\", \"vau\"]\n",
    "\n",
    "for file in glob.glob('./Languages/lojban/Wiki/*.txt'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    sentences = re.split(r\"(\\.i\\s|ni'o|ja'o|vau)\", text)\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(r'(^|\\W)[A-Z][a-z]+', '', sentence)\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        sentence = re.sub(r'{\\\\.*}', '', sentence)\n",
    "        s = ' '.join([re.sub(alpha, '', word.strip()) for word in sentence.split()])\n",
    "        if (len(s) > 0) and (s not in sep):\n",
    "            jbo_filtered.append(s)\n",
    "            \n",
    "with open('./Languages/lojban/Corpus/sentencesfromtsv.txt', 'r', encoding='utf-8') as f:\n",
    "    sentences = f.readlines()\n",
    "    f.close()\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(r\"(\\.i\\s|ni'o|ja'o|vau)\", \"\", sentence)\n",
    "    sentence = re.sub(r'(^|\\W)[A-Z][a-z]+', '', sentence)\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    s = ' '.join([re.sub(alpha, '', word.strip()) for word in sentence.split()])\n",
    "    if (len(s) > 0) and (s not in sep):\n",
    "        jbo_filtered.append(s)\n",
    "\n",
    "with open('Lojban.txt', 'w', encoding='utf-8') as out:\n",
    "    out.write('\\n'.join(jbo_filtered))\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klingon\n",
    "\n",
    "kl_filtered = []\n",
    "\n",
    "for file in glob.glob('./Languages/klingon/wiki/*.txt'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    sentences = [s.strip() for s in text.split('.') if len(s.strip()) >= 3]\n",
    "    kl_filtered.extend(sentences)\n",
    "    \n",
    "with open('./Klingon2.txt', 'w') as out:\n",
    "    out.write('\\n'.join(kl_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
