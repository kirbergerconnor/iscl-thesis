{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, urllib, unicodedata, os.path, glob, string, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = r\"(\\.|!|\\?|;|\\n|。|！|？|；|…|　|!|؟|؛)+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperanto\n",
    "\n",
    "eo_out = open('Esperanto.pkl', 'wb')\n",
    "eo_filtered = []\n",
    "\n",
    "for file in glob.glob('./esperanto/Wiki/*.txt'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    sentences = re.split(r'[.!?]', unicodedata.normalize('NFKC', text))\n",
    "    alpha = 'abcĉdefgĝhĥijĵklmnoprsŝtuŭvz'\n",
    "    for sentence in sentences:\n",
    "        s = ' '.join([word.lower() for word in sentence.split() if all(letter in alpha for letter in word.lower())])\n",
    "        if len(s) > 0:\n",
    "            eo_filtered.append(s)\n",
    "\n",
    "pickle.dump(eo_filtered, eo_out)\n",
    "eo_out.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interlingua\n",
    "\n",
    "ia_out = open('Interlingua.pkl', 'wb')\n",
    "ia_filtered = []\n",
    "\n",
    "for file in glob.glob('./interlingua/Wiki/*.txt'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    sentences = re.split(r'[.!?]\\s', unicodedata.normalize('NFKC', text))\n",
    "    alpha = r'[^a-z]'\n",
    "    for sentence in sentences:\n",
    "        s = ' '.join([re.sub(alpha, '', word.strip().lower()) for word in sentence.split()])\n",
    "        if len(s) > 0:\n",
    "            ia_filtered.append(s)\n",
    "\n",
    "pickle.dump(ia_filtered, ia_out)\n",
    "ia_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lingua Franca Nova\n",
    "\n",
    "lfn_out = open('Lfn.pkl', 'wb')\n",
    "lfn_filtered = []\n",
    "\n",
    "for file in glob.glob('./lfn/Wiki/*.txt'):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    sentences = re.split(r'[.!?]\\s', unicodedata.normalize('NFKC', text))\n",
    "    alpha = r'[^a-z]'\n",
    "    for sentence in sentences:\n",
    "        s = ' '.join([re.sub(alpha, '', word.strip().lower()) for word in sentence.split()])\n",
    "        if len(s) > 0:\n",
    "            lfn_filtered.append(s)\n",
    "\n",
    "pickle.dump(lfn_filtered, lfn_out)\n",
    "lfn_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lojban\n",
    "\n",
    "# jbo_filtered = []\n",
    "\n",
    "# for file in glob.glob('./lojban/Wiki/*.txt'):\n",
    "#     with open(file, 'r', encoding='utf-8') as f:\n",
    "#         text = f.read()\n",
    "#         f.close()\n",
    "#     sentences = \n",
    "#     alpha = 'abcdefgijklmnoprstuvxyz\\'\\.\\,'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
