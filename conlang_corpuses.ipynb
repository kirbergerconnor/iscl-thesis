{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, urllib, unicodedata, os.path, glob, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from string import digits, punctuation\n",
    "from io import StringIO\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = punctuation + digits + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lingua France Nova\n",
    "\n",
    "lfn_wiki = requests.get(\"https://lfn.wikipedia.org/wiki/Lingua_franca_nova\")\n",
    "lfn_soup = BeautifulSoup(lfn_wiki.content, 'html.parser')\n",
    "lfn_res = lfn_soup.find(\"div\", attrs={\"class\": \"mw-body-content\"}).findAll(\"p\")\n",
    "lfn_res = [p_tag.get_text() for p_tag in lfn_res]\n",
    "lfn_res = [''.join(word.strip(exclude).lower() for word in lines) for lines in lfn_res]\n",
    "lfn_res = [line for line in lfn_res if line]\n",
    "\n",
    "with open('corpo.txt', 'r', encoding='utf-8') as lfn_corpo_f:\n",
    "    lfn_corpo = lfn_corpo_f.readlfn_corpo()\n",
    "    lfn_corpo = [line.strip(exclude).lower() for line in lfn_corpo if not line.startswith('@') and not line.startswith('=') and not line[0].isdigit()]\n",
    "    lfn_corpo_f.close()\n",
    "    \n",
    "lfn_output = lfn_res + lfn_corpo\n",
    "\n",
    "with open('Lfn.txt', 'w', encoding='utf-8') as lfn_out_f:\n",
    "    lfn_out_f.writelines(\"\\n\".join(lfn_output))\n",
    "    lfn_out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lojban\n",
    "\n",
    "with open('jb2en.tsv') as lojban_f:\n",
    "    lojban_text = lojban_f.readlines()\n",
    "\n",
    "lojban_text = [line.split('\\t')[0].strip() for line in lojban_text]\n",
    "\n",
    "lojban_wiki = requests.get(\"https://jbo.wikipedia.org/wiki/lo_jbobau\")\n",
    "lojban_soup = BeautifulSoup(lojban_wiki.content, 'html.parser')\n",
    "lojban_res = lojban_soup.find(\"div\", attrs={\"class\": \"mw-parser-output\"}).findAll(\"p\")\n",
    "lojban_res = [p_tag.get_text().strip().replace(\"\\n\", \" \").lower() for p_tag in lojban_res]\n",
    "lojban_output = lojban_text + lojban_res\n",
    "\n",
    "with open('Lojban.txt', 'w', encoding='utf-8') as lojban_out_f:\n",
    "    lojban_out_f.writelines(\"\\n\".join(lojban_output))\n",
    "    lojban_out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interlingua\n",
    "\n",
    "# response = requests.get(\"https://ia.wikipedia.org/wiki/Interlingua\")\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# res = soup.find(\"div\", attrs={\"class\": \"mw-body-content\"}).findAll(\"p\")\n",
    "# res = [unicodedata.normalize('NFKC', p_tag.get_text()) for p_tag in res]\n",
    "# res = [''.join(word.lower() for word in lines) for lines in res]\n",
    "with open('English_and_Interlingua_Parallel_Sentences.txt', 'r', encoding='utf-8') as ia_file:\n",
    "    ia_sentences = ia_file.readlines()\n",
    "    ia_sentences = [line.lower().strip('[ina]').strip() for line in ia_sentences if '[ENG]' not in line]\n",
    "    ia_sentences = [line for line in ia_sentences if line]\n",
    "    ia_file.close()\n",
    "# output = res + lines\n",
    "with open('Interlingua.txt', 'w', encoding='utf-8') as ia_out_f:\n",
    "    ia_out_f.writelines(\"\\n\".join(ia_sentences))\n",
    "    ia_out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperanto\n",
    "\n",
    "# response = requests.get(\"https://eo.wikipedia.org/wiki/Esperanto\")\n",
    "# wiki_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# wiki_res = wiki_soup.find(\"div\", attrs={\"class\": \"mw-parser-output\"}).findAll(\"p\")\n",
    "# wiki_res = [p_tag.get_text().strip().replace(\"\\n\", \" \").lower() for p_tag in wiki_res]\n",
    "\n",
    "# content = urllib.request.urlopen('https://www.sacred-texts.com/bib/wb/esp/co1.htm')\n",
    "\n",
    "# read_content = content.read()\n",
    "# bible_soup = BeautifulSoup(read_content, 'html.parser')\n",
    "\n",
    "# [a_tag.decompose() for a_tag in bible_soup.find_all('a')]\n",
    "# bible_res = [p_tag.get_text().strip().replace(\"\\n\", \" \").lower() for p_tag in bible_soup.find_all(\"p\")]\n",
    "eo_sentences = []\n",
    "\n",
    "for eo_file in glob.glob(os.path.join('./esperanto_corpus', '*.html')):\n",
    "    with open(eo_file) as eo_html_f:\n",
    "        eo_soup = BeautifulSoup(eo_html_f)\n",
    "        remove = eo_soup.find(\"div\", attrs={\"class\": \"tekstokapo\"})\n",
    "        remove.extract()\n",
    "        eo_res = [unicodedata.normalize('NFKC', tag.get_text().lower()) for tag in eo_soup.find(\"div\", attrs={\"class\": \"tekstarteksto\"}).find_all('p')]\n",
    "        eo_sentences += eo_res\n",
    "        eo_html_f.close()\n",
    "\n",
    "with open('Esperanto.txt', 'w', encoding='utf-8') as eo_out_f:\n",
    "    eo_out_f.writelines(\"\\n\".join(eo_sentences))\n",
    "    eo_out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lingwa de Planeta\n",
    "\n",
    "# Humanist Speech PDF\n",
    "ldp_outstr = StringIO()\n",
    "with open('humanist_speech_trans.pdf', 'rb') as ldp_f_1:\n",
    "    extract_text_to_fp(ldp_f_1, ldp_outstr, laparams=LAParams(),\n",
    "                       output_type='html', codec=None)\n",
    "    \n",
    "ldp_soup_1 = BeautifulSoup(ldp_outstr.getvalue())\n",
    "ldp_text_1 = [sen.get_text().strip().replace(\"\\n\", \" \").lower() for sen in ldp_soup_1.find_all(\"span\", style=\"font-family: font00000000296ca43f; font-size:12px\") if \"\\nPage\" not in sen.get_text()]\n",
    "\n",
    "\n",
    "# Animal Farm PDF\n",
    "ldp_outstr_2 = StringIO()\n",
    "with open('animal_farm_trans.pdf', 'rb') as ldp_f_2:\n",
    "    extract_text_to_fp(ldp_f_2, ldp_outstr_2, laparams=LAParams(),\n",
    "                       output_type='html', codec=None)\n",
    "\n",
    "ldp_soup_2 = BeautifulSoup(ldp_outstr_2.getvalue())\n",
    "ldp_text_2 = [sen.get_text().strip().replace(\"\\n\", \" \").lower() for sen in ldp_soup_2.find_all([\"spa\"], style=\"font-family: font00000000296cac0e; font-size:12px\") if \"\\nPage\" not in sen.get_text()]\n",
    "\n",
    "with open('LdP.txt', 'w', encoding='utf-8') as ldp_out_f:\n",
    "    ldp_out_f.write(\" \".join(ldp_text_1))\n",
    "    ldp_out_f.write(\" \".join(ldp_text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klingon\n",
    "\n",
    "klingon_content = urllib.request.urlopen('http://klingon.wiki/En/TheKlingonWayPhrases')\n",
    "read_content = klingon_content.read()\n",
    "klingon_soup = BeautifulSoup(read_content, 'html.parser')\n",
    "\n",
    "with open('Klingon.txt', 'w', encoding='utf-8') as klingon_f:\n",
    "    for sen in klingon_soup.find_all(\"b\", limit=156):\n",
    "        klingon_f.write(sen.get_text() + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dothraki\n",
    "\n",
    "dothraki_outstr = StringIO()\n",
    "with open('Dothraki_dic.pdf', 'rb') as dothraki_f:\n",
    "    extract_text_to_fp(dothraki_f, dothraki_outstr, laparams=LAParams(),\n",
    "                       output_type='html', codec=None)\n",
    "    \n",
    "dothraki_soup = BeautifulSoup(dothraki_outstr.getvalue())\n",
    "dothraki_res = [word.get_text() for word in dothraki_soup.find_all([\"span\"], style=\"font-family: URWPalladioL-Bold; font-size:10px\", limit=1455) if \"\\n\" not in word.get_text()] \n",
    "\n",
    "with open('Dothraki.txt', 'w', encoding='utf-8') as dothraki_out_f:\n",
    "    dothraki_out_f.write(\" \".join(dothraki_res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
