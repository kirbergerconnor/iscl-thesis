{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and preprocess text data for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_sentences(content):\n",
    "#     # Remove opening html tag + article title\n",
    "#     trimmed_open = re.sub(r'<doc\\b[^>]*>\\s*(?:.*\\n)?', '', content)\n",
    "#     # Remove closing html tag\n",
    "#     trimmed_close = re.sub(r'<\\/doc\\b[^>]*>', '', trimmed_open)\n",
    "#     # Remove single-word subheaders\n",
    "#     trimmed_sub = re.sub(r'(?<=\\n)\\w+\\.(?=\\n)', '', trimmed_close)\n",
    "#     # Split at periods while trying to overlook abbreviations\n",
    "#     return re.split(r'(?<!\\w\\.\\w)(?<!\\w\\.)(?<!Sr)(?<!Mr)(?<!Ms)(?<!Mrs)(?<!Jr)(?<!Dr)\\.(?!\\w)', trimmed_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick test\n",
    "# with open('/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/WikiExtractor/lfn_wiki.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "# cleaned = clean_sentences(text)\n",
    "# alpha = re.compile(r'[^a-z]')\n",
    "# with open('/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/lfn_example.txt', 'w', encoding='utf-8') as out:\n",
    "#     for sentence in cleaned:\n",
    "#         s = ' '.join([re.sub(alpha, '', word.strip().lower()) for word in sentence.split()])\n",
    "#         s = re.sub(r'\\s+', ' ', s).strip()\n",
    "#         if len(s.split()) > 1:\n",
    "#             out.write(s + '.' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory with extracted files\n",
    "# maindir = '/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/HtmlWiki'\n",
    "# wikidirs = [sub for sub in os.listdir(maindir) if sub != '.DS_Store']\n",
    "# # Dict of languages + their corresponding alphabets\n",
    "# alphabets = {\n",
    "#     'lfn': re.compile(r'[^a-z]'), \n",
    "#     'ia': re.compile(r'[^a-z]'), \n",
    "#     'eo': 'abcĉdefgĝhĥijĵklmnoprsŝtuŭvz', \n",
    "#     'de': 'abcdefghijklmnopqrstuvwxyzäöüß', \n",
    "#     'fr': 'abcdefghijklmnopqrstuvwxyzéàèùëüïâêîôûçæœ', \n",
    "#     'en': re.compile(r'[^a-z]')\n",
    "#     }\n",
    "\n",
    "# # Estimated smallest wc (from lfn corpus)\n",
    "# min_wc = 1265000\n",
    "\n",
    "# for wiki in wikidirs:\n",
    "#     wiki_fullpath = os.path.join(maindir, wiki)\n",
    "#     lang = os.path.splitext(os.path.basename(wiki))[0].split('_')[0]\n",
    "#     current_wc = 0\n",
    "#     if lang in alphabets:\n",
    "#         alpha = alphabets[lang]\n",
    "#     with open(f'/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/current_corpora/{lang}_wiki_html.txt', 'w', encoding='utf-8') as out:\n",
    "#         for root, dirs, files in os.walk(wiki_fullpath):\n",
    "#             for file in files:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 # Make sure only corpora files \n",
    "#                 if '/wiki' in file_path:\n",
    "#                     with open(file_path) as f: \n",
    "#                         content = f.read()\n",
    "#                         f.close()\n",
    "#                     sentences = clean_sentences(content)\n",
    "#                     # Check if alpha in dict is regex\n",
    "#                     if isinstance(alpha, re.Pattern):\n",
    "#                         for sentence in sentences:\n",
    "#                             # Remove all chars not language's alphabet\n",
    "#                             s = ' '.join([re.sub(alpha, '', word.strip().lower()) for word in sentence.split()])\n",
    "#                             # Remove leftover whitespaces\n",
    "#                             s = re.sub(r'\\s+', ' ', s).strip()\n",
    "#                             if (len(s.split()) + current_wc) < min_wc:\n",
    "#                                 current_wc += len(s.split())\n",
    "#                                 out.write(s + '.' + '\\n')\n",
    "#                             else:\n",
    "#                                 break\n",
    "#                     else:\n",
    "#                         for sentence in sentences:\n",
    "#                             # Remove all chars not language's alphabet\n",
    "#                             s = ' '.join([word.lower() for word in sentence.split() if all(letter in alpha for letter in word.lower())])\n",
    "#                             # Remove leftover whitespaces\n",
    "#                             s = re.sub(r'\\s+', ' ', s).strip()\n",
    "#                             if (len(s.split()) + current_wc) < min_wc:\n",
    "#                                 current_wc += len(s.split())\n",
    "#                                 out.write(s + '.' + '\\n')\n",
    "#                             else:\n",
    "#                                 break\n",
    "#     out.close()\n",
    "#     print(f'{lang}_wiki_html.txt finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using /iscl-thesis/WikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, string\n",
    "\n",
    "def clean_sentences(content):\n",
    "    # Remove page titles\n",
    "    no_titles = re.sub(r'(?<=\\s\\n).+:\\n', '', content)\n",
    "    # Remove section heaeders\n",
    "    no_headers = re.sub(r'.+\\/\\/\\/\\/\\/\\n', '', no_titles)\n",
    "    # Remove picture links\n",
    "    no_pics = re.sub(r'(?<=\\n)(\\w+\\|)+.+[^\\.]\\n', '', no_headers)\n",
    "    # Remove fragments\n",
    "    no_fragments = re.sub(r'(?<=\\n)\\w+\\:\\w+.*[^\\.](?=\\n)', '', no_pics)\n",
    "    # Remove other links\n",
    "    no_links = re.sub(r'http[^\\s\\n]+(?=\\s|\\n)', '', no_fragments)\n",
    "    # Remove paranthesis\n",
    "    no_paranthesis = re.sub(r'\\([^\\)]+\\)', '', no_links)\n",
    "    # Removes braces\n",
    "    no_braces = re.sub(r'\\[+\\w+[^\\]]*\\]+', '', no_paranthesis)\n",
    "    # Remove lists\n",
    "    no_lists = re.sub(r'(?<=\\n)[^\\.\\n]+\\:\\s?\\n', '', no_braces)\n",
    "    # Remove html tags\n",
    "    no_html = re.sub(r'\\<[^\\>]+\\>', '', no_lists)\n",
    "    # Remove number fragments\n",
    "    no_num = re.sub(r'\\d+(?=\\n)', '', no_html)\n",
    "    # Split at periods while trying to overlook abbreviations\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w)(?<!\\w\\.)(?<!Sr)(?<!Mr)(?<!Ms)(?<!Mrs)(?<!Jr)(?<!Dr)\\.(?!\\w)', no_num)\n",
    "    # Remove punctuation, numbers, make lowercase\n",
    "    return [s.translate(str.maketrans('', '', string.punctuation + string.digits)).lower() for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tl corpus finished\n",
      "oc corpus finished\n",
      "af corpus finished\n",
      "eo corpus finished\n",
      "en corpus finished\n",
      "tr corpus finished\n",
      "io corpus finished\n",
      "de corpus finished\n",
      "fr corpus finished\n",
      "id corpus finished\n",
      "vi corpus finished\n",
      "sv corpus finished\n",
      "ia corpus finished\n",
      "nl corpus finished\n",
      "es corpus finished\n",
      "da corpus finished\n",
      "it corpus finished\n",
      "pl corpus finished\n",
      "lfn corpus finished\n",
      "fi corpus finished\n",
      "is corpus finished\n",
      "hu corpus finished\n"
     ]
    }
   ],
   "source": [
    "# Directory with extracted files\n",
    "maindir = '/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/WikiExtractorCorpora/'\n",
    "# Dict of languages + their corresponding alphabets\n",
    "alphabets = {\n",
    "    # Lingua Franca Nova\n",
    "    'lfn': 'abcdefghijklmnopqrstuvwxyz', \n",
    "    # Interlingua\n",
    "    'ia': 'abcdefghijklmnopqrstuvwxyz', \n",
    "    # Esperanto\n",
    "    'eo': 'abcĉdefgĝhĥijĵklmnoprsŝtuŭvz', \n",
    "    # German\n",
    "    'de': 'abcdefghijklmnopqrstuvwxyzäöüß', \n",
    "    # French\n",
    "    'fr': 'abcdefghijklmnopqrstuvwxyzéàèùëüïâêîôûçæœ', \n",
    "    # English\n",
    "    'en': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Finnish\n",
    "    'fi': 'abcdefghijklmnopqrstuvwxyzšžåäö',\n",
    "    # Tagalog\n",
    "    'tl': 'abcdefghijklmnopqrstuvwxyzñ',\n",
    "    # Turkish\n",
    "    'tr': 'abcçdefgğhıijklmnoöprsştuüvyz',\n",
    "    # Vietnamese\n",
    "    'vi': 'aáàảãạăắằẳẵặâấầẩẫậbcdđeéèẻẽẹêếềểễệfghiíìỉĩịjklmnoóòỏõọôốồổỗộơớờởỡợpqrstuúùủũụưứừửữựvwxyýỳỷỹỵz',\n",
    "    # Polish\n",
    "    'pl': 'aąbcćdeęfghijklłmnńoópqrsśtuvwxyzźż',\n",
    "    # Indonesian\n",
    "    'id': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Ido\n",
    "    'io': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Italian\n",
    "    'it': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Dutch\n",
    "    'nl': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Occitan\n",
    "    'oc': 'abcdefghijklmnopqrstuvwxyzàèòáéíóúïüç',\n",
    "    # Danish\n",
    "    'da': 'abcdefghijklmnopqrstuvwxyzæøå',\n",
    "    # Swedish\n",
    "    'sv': 'abcdefghijklmnopqrstuvwxyzåäö',\n",
    "    # Hungarian\n",
    "    'hu': 'aábccsddzdzseéfggyhiíjkllymnnyoóöőpqrsszttyuúüűvwxyzzs',\n",
    "    # Spanish\n",
    "    'es': 'abcdefghijklmnopqrstuvwxyzñ',\n",
    "    # Afrikaans\n",
    "    'af': 'aáäbcdeéèêëfghiíîïjklmnŉoóôöpqrstuúûüvwxyýz',\n",
    "    # Icelandic\n",
    "    'is': 'aábdðeéfghiíjklmnoóprstuúvxyýþæö',\n",
    "    }\n",
    "\n",
    "# LFN corpus is the smallest size based on word count, at about 650000 words, so shrink all other corpora to about the same\n",
    "max_wc = 650000\n",
    "\n",
    "for file in os.listdir(maindir):\n",
    "    if file.endswith('.txt'):\n",
    "        fullpath = os.path.join(maindir, file)\n",
    "        lang = os.path.splitext(os.path.basename(file))[0].split('_')[0]\n",
    "        current_wc = 0\n",
    "        if lang in alphabets:\n",
    "            alpha = set(alphabets[lang])\n",
    "        with open(f'/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/current_corpora/{lang}_wiki_extractor.txt', 'w', encoding='utf-8') as out:\n",
    "            with open(fullpath, 'r', encoding='utf-8') as f: \n",
    "                content = f.read()\n",
    "                f.close()\n",
    "            sentences = clean_sentences(content)\n",
    "            for sentence in sentences:\n",
    "                # Remove all chars not language's alphabet\n",
    "                s = ''.join(char if char in alpha or char.isspace() else '' for char in sentence)\n",
    "                # Remove leftover whitespaces\n",
    "                s = re.sub(r'\\s+', ' ', s).strip()\n",
    "                # Remove sentence fragments containing only 1 word\n",
    "                if len(s.split()) > 1:\n",
    "                    if (len(s.split()) + current_wc) < max_wc:\n",
    "                        current_wc += len(s.split())\n",
    "                        out.write(s + '\\n')\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        print(f'{lang} corpus finished')\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
