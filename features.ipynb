{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from adjustText import adjust_text\n",
    "from collections import defaultdict\n",
    "\n",
    "dir = '/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/2024_corpora/'\n",
    "files = [os.path.join(dir, file) for file in os.listdir(dir) if file.endswith('.txt')]\n",
    "\n",
    "languages = {\n",
    "    'id': 'nat', \n",
    "    'tl': 'nat', \n",
    "    'tr': 'nat', \n",
    "    'en': 'nat', \n",
    "    'de': 'nat',\n",
    "    'fr': 'nat',\n",
    "    'eo': 'con',\n",
    "    'lfn': 'con',\n",
    "    'ia': 'con',\n",
    "    'io': 'con',\n",
    "    'pl': 'nat',\n",
    "    'vi': 'nat',\n",
    "    'fi': 'nat',\n",
    "    'it': 'nat',\n",
    "    'af': 'nat',\n",
    "    'nl': 'nat',\n",
    "    'es': 'nat',\n",
    "    'oc': 'nat',\n",
    "    'da': 'nat',\n",
    "    'sv': 'nat',\n",
    "    'is': 'nat',\n",
    "    'hu': 'nat',\n",
    "    'vo': 'con',\n",
    "    'avk': 'con'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTR and MATTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTR\n",
    "def calculate_ttr(file):\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    tokens = text.split()\n",
    "    total_tokens = len(tokens)\n",
    "    types = set(tokens)\n",
    "    total_types = len(types)\n",
    "    ttr = (total_types / total_tokens) * 100\n",
    "    return ttr\n",
    "\n",
    "ttr_values = {}\n",
    "    \n",
    "# MATTR\n",
    "def calculate_mattr(file, window_size):\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    tokens = text.split()\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    ttr_values = []\n",
    "    for i in range(total_tokens - window_size + 1):\n",
    "        window = tokens[i:i + window_size]\n",
    "        types = set(window)\n",
    "        total_types = len(types)\n",
    "        ttr = total_types / window_size\n",
    "        ttr_values.append(ttr)\n",
    "\n",
    "    mattr = sum(ttr_values) / len(ttr_values)\n",
    "    return mattr\n",
    "\n",
    "mattr_values = {}\n",
    "window_size = 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character and Word Distribution Entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char Dist Entropy\n",
    "def log2(number):\n",
    "    return log(number) / log(2)\n",
    "\n",
    "def calculate_char_entropy(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    char_freq = {}\n",
    "    for char in text:\n",
    "        if char in char_freq:\n",
    "            char_freq[char] += 1\n",
    "        else:\n",
    "            char_freq[char] = 1\n",
    "            \n",
    "    length_sum = 0.0\n",
    "    for char in char_freq:\n",
    "        p = float(char_freq[char]) / len(text)\n",
    "        length_sum += p * log2(p)\n",
    "        \n",
    "    return -length_sum\n",
    "\n",
    "char_entropies = {}\n",
    "    \n",
    "# Word Dist Entropy\n",
    "def log2(number):\n",
    "    return log(number) / log(2)\n",
    "\n",
    "def calculate_word_entropy(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    word_freq = {}\n",
    "    total_words = 0\n",
    "    for word in text.split():\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "        total_words += 1\n",
    "            \n",
    "    length_sum = 0.0\n",
    "    for word in word_freq:\n",
    "        p = float(word_freq[word]) / total_words\n",
    "        length_sum += p * log2(p)\n",
    "        \n",
    "    return -length_sum\n",
    "\n",
    "word_entropies = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Word/Sentence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_word_len(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    f.close()\n",
    "    words = text.split()    \n",
    "    return (sum(len(word) for word in words)) / len(words)\n",
    "\n",
    "def calculate_avg_sentence_len(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    # total_words = 0\n",
    "    # total_characters = 0\n",
    "    # for sentence in text:\n",
    "    #     words = sentence.split()\n",
    "    #     total_words += len(words)\n",
    "    #     total_characters += sum(len(word) for word in words)\n",
    "    \n",
    "    # return total_words / len(text), total_characters / len(text)\n",
    "    \n",
    "    sentence_lengths = []\n",
    "    for sentence in text:\n",
    "        words = sentence.split()\n",
    "        sentence_lengths.append(len(words))\n",
    "        \n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "avg_word_length = {}\n",
    "avg_sentence_length = {}\n",
    "\n",
    "for file in files:\n",
    "    lang = os.path.splitext(os.path.basename(file))[0].split('_')[0]\n",
    "    ttr_values[lang] = calculate_ttr(file)\n",
    "    mattr_values[lang] = calculate_mattr(file, window_size)\n",
    "    char_entropies[lang] = calculate_char_entropy(file)\n",
    "    word_entropies[lang] = calculate_word_entropy(file)\n",
    "    avg_word_length[lang] = calculate_avg_word_len(file)\n",
    "    avg_sentence_length[lang] = calculate_avg_sentence_len(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Features Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Corpus Type  Text Entr  Avg Word Length  Avg Sentence Length        TTR  \\\n",
      "0      id  nat   3.517539         6.173187            18.164432   5.781932   \n",
      "1      tl  nat   3.824064         5.119253            21.101625   7.593148   \n",
      "2      tr  nat   4.113557         6.629588            14.458380  14.097413   \n",
      "3      en  nat   4.115928         5.087280            21.301075   6.078659   \n",
      "4      de  nat   3.964652         6.205757            16.907410  12.127552   \n",
      "5      fr  nat   3.496586         5.159560            23.120339   7.461471   \n",
      "6      eo  con   3.857892         5.174506            18.909085  10.708038   \n",
      "7     lfn  con   3.935966         4.220976            19.531596   5.063124   \n",
      "8      ia  con   3.336037         5.050308            19.547488   6.880044   \n",
      "9      io  con   1.156939         4.593711            14.483861   3.432594   \n",
      "10     pl  nat   4.316066         6.247614            14.950804  14.889753   \n",
      "11     vi  nat   4.000555         3.498411            29.834620   1.748688   \n",
      "12     fi  nat   3.915311         7.873823            11.968653  20.409242   \n",
      "13     it  nat   4.003349         5.455254            25.727406   8.505096   \n",
      "14     af  nat   4.088097         5.067440            20.496275   6.987209   \n",
      "15     nl  nat   3.813483         5.418616            18.193808   8.559247   \n",
      "16     es  nat   3.502263         4.978420            25.314554   7.084533   \n",
      "17     oc  nat   2.963280         5.215132            18.659973   7.184626   \n",
      "18     da  nat   4.342160         5.345745            16.466257  10.517318   \n",
      "19     sv  nat   4.169627         5.596711            17.321914  11.031146   \n",
      "20     is  nat   4.642987         5.375332            15.054723  11.727395   \n",
      "21     hu  nat   4.422766         6.241618            15.781792  16.234407   \n",
      "22     vo  con   1.191632         5.072429            11.266077   2.454766   \n",
      "23    avk  con   3.084704         5.059741            12.823762   8.152737   \n",
      "\n",
      "       MATTR  Char Dist Entr  Word Dist Entr  Rev Lex Entr  Lex Entr  \n",
      "0   0.698615        4.072348       11.142288      1.975964  1.955565  \n",
      "1   0.611370        3.895053        9.990761      1.917461  1.884069  \n",
      "2   0.828179        4.386410       13.151038      1.656299  1.562062  \n",
      "3   0.697320        4.166815       10.673342      1.981161  1.925805  \n",
      "4   0.770622        4.229662       11.600880      1.666065  1.608298  \n",
      "5   0.721291        4.179083       10.710805      1.864765  1.792511  \n",
      "6   0.691985        4.164466       10.923163      1.893018  1.801390  \n",
      "7   0.600943        3.912044        9.316262      2.113744  2.026918  \n",
      "8   0.607494        4.032422       10.004512      1.906465  1.820672  \n",
      "9   0.557337        4.076890        8.054660      2.069359  1.985350  \n",
      "10  0.824694        4.553291       12.904856      1.685317  1.650713  \n",
      "11  0.732118        4.855021        9.716517      2.386668  2.420569  \n",
      "12  0.840939        4.144350       13.728718      1.631041  1.546864  \n",
      "13  0.763998        4.028519       11.308415      1.786493  1.671975  \n",
      "14  0.645126        4.071800        9.993007      1.913512  1.838994  \n",
      "15  0.693924        4.116845       10.592579      1.866150  1.810840  \n",
      "16  0.674374        4.046247       10.326538      1.864251  1.759183  \n",
      "17  0.715421        4.172687       10.546377      1.934265  1.870505  \n",
      "18  0.737119        4.196603       11.273980      1.870206  1.807829  \n",
      "19  0.755942        4.294349       11.488269      1.836367  1.774936  \n",
      "20  0.747371        4.468399       11.511686      1.795803  1.727692  \n",
      "21  0.775506        4.543481       12.442873      1.726853  1.670353  \n",
      "22  0.621875        4.256367        7.665711      2.135370  2.085906  \n",
      "23  0.582088        4.186332       10.287315      2.065925  2.011161  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1w/_90xn6_52fq9d37y9_7snxr40000gn/T/ipykernel_17372/2676000107.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_entropy_df.rename(columns={'val_perplexity':'Text Entr'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "calculations = {\n",
    "    'Avg Word Length': avg_word_length,\n",
    "    'Avg Sentence Length': avg_sentence_length,\n",
    "    'TTR': ttr_values,\n",
    "    'MATTR': mattr_values,\n",
    "    'Char Dist Entr': char_entropies,\n",
    "    'Word Dist Entr': word_entropies\n",
    "}\n",
    "for label, dic in calculations.items():\n",
    "    calculations[label] = pd.Series(dic)\n",
    "temp = pd.DataFrame(calculations)\n",
    "calculations_df = temp.copy()\n",
    "calculations_df['corpus'] = calculations_df.index\n",
    "calculations_df = calculations_df[['corpus', 'Avg Word Length', 'Avg Sentence Length', 'TTR', 'MATTR', 'Char Dist Entr', 'Word Dist Entr']]\n",
    "# Getting the validation loss for the last epoch and last step for each language from the torch_rnn_results.csv file (for text entropy) and using val_perplexity as entropy value \n",
    "torch_rnn_df = pd.read_csv('./results/torch_rnn_results.csv', index_col=0)\n",
    "dfs = []\n",
    "for lang in languages:\n",
    "    temp = torch_rnn_df.loc[(torch_rnn_df['epoch'] == 20) & (torch_rnn_df['corpus'] == lang)]\n",
    "    temp = temp[temp['step'] == temp['step'].max()]\n",
    "    dfs.append(temp)\n",
    "ref = pd.concat(dfs)\n",
    "text_entropy_df = ref[['corpus', 'val_perplexity']]\n",
    "text_entropy_df.insert(1, 'Type', ref['corpus'].apply(lambda x: languages.get(x)))\n",
    "text_entropy_df.rename(columns={'val_perplexity':'Text Entr'}, inplace=True)\n",
    "reverse_lex_entropy_df, lex_entropy_df = pd.read_csv('./results/reverse_lex_entropy_hist_df.csv'), pd.read_csv('./results/lex_entropy_hist_df.csv')\n",
    "reverse_lex_entropy_df.rename(columns={'val_loss':'Rev Lex Entr'}, inplace=True)\n",
    "lex_entropy_df.rename(columns={'val_loss':'Lex Entr'}, inplace=True)\n",
    "features_df = pd.merge(text_entropy_df, calculations_df, on='corpus', how='left')\n",
    "features_df = pd.merge(features_df, reverse_lex_entropy_df[['corpus', 'Rev Lex Entr']], on='corpus', how='left')\n",
    "features_df = pd.merge(features_df, lex_entropy_df[['corpus', 'Lex Entr']], on='corpus', how='left')\n",
    "features_df.rename(columns={'corpus':'Corpus'}, inplace=True)\n",
    "features_df.to_csv('./results/features_df.csv', columns=['Corpus', 'Type', 'Avg Word Length', 'Avg Sentence Length', 'TTR', 'MATTR', 'Char Dist Entr', 'Word Dist Entr', 'Text Entr', 'Lex Entr', 'Rev Lex Entr'], index=False)\n",
    "print(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA of Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINING NUMBER OF PRINCIPLE COMPONENTS TO USE FOR CALCULATIONS DATA (from https://medium.com/analytics-vidhya/feature-extraction-techniques-pca-lda-and-t-sne-df0459c723aa)\n",
    "\n",
    "numeric_df = calculations_df.select_dtypes(include='number')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit_transform(scaled_data)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')  # for each component\n",
    "plt.title('Calculations Dataset Explained Variance')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING #\n",
    "\n",
    "numeric_df = calculations_df.select_dtypes(include='number')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df.index = calculations_df.index\n",
    "\n",
    "# print(pca_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', data=pca_df, hue=pca_df.index, s=50)\n",
    "\n",
    "texts = []\n",
    "for i, language in enumerate(pca_df.index):\n",
    "    texts.append(plt.annotate(language, (pca_df['PC1'][i], pca_df['PC2'][i])))\n",
    "    \n",
    "adjust_text(texts)\n",
    "\n",
    "lang_labels = {\n",
    "    'sv': 'Swedish', \n",
    "    'id': 'Indonesian', \n",
    "    'eo': 'Esperanto',\n",
    "    'is': 'Icelandic',\n",
    "    'pl': 'Polish',\n",
    "    'da': 'Danish',\n",
    "    'tl': 'Tagalog',\n",
    "    'ia': 'Interlingua',\n",
    "    'fi': 'Finnish',\n",
    "    'vi': 'Vietnamese',\n",
    "    'hu': 'Hungarian',\n",
    "    'nl': 'Dutch',\n",
    "    'lfn': 'Lingua Franca Nova',\n",
    "    'es': 'Spanish',\n",
    "    'io': 'Ido',\n",
    "    'af': 'Afrikaans',\n",
    "    'fr': 'French',\n",
    "    'it': 'Italian',\n",
    "    'tr': 'Turkish',\n",
    "    'de': 'German',\n",
    "    'oc': 'Occitan',\n",
    "    'en': 'English',\n",
    "    'vo': 'Volapük',\n",
    "    'avk': 'Kotava'\n",
    "    }\n",
    "legend_labels = [lang_labels[abb] for abb in pca_df.index]\n",
    "    \n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA of TTR, MATTR, Char and Word Distribution Entropies')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Language', labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA of All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINING NUMBER OF PRINCIPLE COMPONENTS TO USE FOR ALL FEATURES DATA (from https://medium.com/analytics-vidhya/feature-extraction-techniques-pca-lda-and-t-sne-df0459c723aa)\n",
    "\n",
    "numeric_df2 = features_df.select_dtypes(include='number')\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaled_data2 = scaler2.fit_transform(numeric_df2)\n",
    "\n",
    "pca2 = PCA()\n",
    "pca2.fit_transform(scaled_data2)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca2.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')  # for each component\n",
    "plt.title('All Features Dataset Explained Variance')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINING NUMBER OF PRINCIPLE COMPONENTS TO USE FOR ALL FEATURES DATA (from https://statisticsglobe.com/scree-plot-pca-python)\n",
    "# Without considering Kaiser's rule\n",
    "\n",
    "numeric_df2 = features_df.select_dtypes(include='number')\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaled_data2 = scaler2.fit_transform(numeric_df2)\n",
    "pca2 = PCA()\n",
    "pca2.fit_transform(scaled_data2)\n",
    "\n",
    "eigenvalues = pca2.explained_variance_\n",
    "prop_var = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(prop_var)+1), \n",
    "                   prop_var, marker='o')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.title('Figure 1: Scree Plot for Proportion of Variance Explained')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINING NUMBER OF PRINCIPLE COMPONENTS TO USE FOR ALL FEATURES DATA (from https://statisticsglobe.com/scree-plot-pca-python)\n",
    "# With considering Kaiser's rule\n",
    "\n",
    "numeric_df2 = features_df.select_dtypes(include='number')\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaled_data2 = scaler2.fit_transform(numeric_df2)\n",
    "pca2 = PCA()\n",
    "pca2.fit_transform(scaled_data2)\n",
    "\n",
    "eigenvalues = pca2.explained_variance_\n",
    "prop_var = eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(eigenvalues)+1), \n",
    "         eigenvalues, marker='o')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Figure 2: Scree Plot for Eigenvalues',)\n",
    "plt.axhline(y=1, color='r', \n",
    "            linestyle='--')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df3 = features_df.select_dtypes(include='number')\n",
    "\n",
    "scaler3 = StandardScaler()\n",
    "scaled_data3 = scaler.fit_transform(numeric_df3)\n",
    "\n",
    "pca3 = PCA(n_components=2)\n",
    "pca_result3 = pca3.fit_transform(scaled_data3)\n",
    "\n",
    "pca_df3 = pd.DataFrame(pca_result3, columns=['PC1', 'PC2'])\n",
    "pca_df3.index = calculations_df.index\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', data=pca_df3, hue=pca_df3.index, s=50)\n",
    "\n",
    "texts = []\n",
    "for i, language in enumerate(pca_df3.index):\n",
    "    texts.append(plt.annotate(language, (pca_df3['PC1'][i], pca_df3['PC2'][i])))\n",
    "    \n",
    "adjust_text(texts)\n",
    "\n",
    "lang_labels = {\n",
    "    'sv': 'Swedish', \n",
    "    'id': 'Indonesian', \n",
    "    'eo': 'Esperanto',\n",
    "    'is': 'Icelandic',\n",
    "    'pl': 'Polish',\n",
    "    'da': 'Danish',\n",
    "    'tl': 'Tagalog',\n",
    "    'ia': 'Interlingua',\n",
    "    'fi': 'Finnish',\n",
    "    'vi': 'Vietnamese',\n",
    "    'hu': 'Hungarian',\n",
    "    'nl': 'Dutch',\n",
    "    'lfn': 'Lingua Franca Nova',\n",
    "    'es': 'Spanish',\n",
    "    'io': 'Ido',\n",
    "    'af': 'Afrikaans',\n",
    "    'fr': 'French',\n",
    "    'it': 'Italian',\n",
    "    'tr': 'Turkish',\n",
    "    'de': 'German',\n",
    "    'oc': 'Occitan',\n",
    "    'en': 'English',\n",
    "    'vo': 'Volapük',\n",
    "    'avk': 'Kotava'\n",
    "    }\n",
    "legend_labels3 = [lang_labels[abb] for abb in pca_df3.index]\n",
    "    \n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA of Features')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Language', labels=legend_labels3, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow https://medium.com/analytics-vidhya/feature-extraction-techniques-pca-lda-and-t-sne-df0459c723aa for plotting cumulative explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA on all feature dimensions\n",
    "# all_dim_df = pd.read_csv('./results/features_df.csv')\n",
    "# data = all_dim_df.drop(columns=['Type'])\n",
    "# data = data.set_index('Corpus')\n",
    "# data.index.names = [None]\n",
    "\n",
    "# # Why scale the data? Why fit_transform?\n",
    "# scaler2 = StandardScaler()\n",
    "# scaled_data2 = scaler2.fit_transform(data)\n",
    "\n",
    "# pca2 = PCA(n_components=0.95)\n",
    "# pca_result2 = pca2.fit_transform(scaled_data2)\n",
    "\n",
    "# pca_df2 = pd.DataFrame(pca_result2, columns=['PC1', 'PC2'])\n",
    "# pca_df2.index = data.index\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(x='PC1', y='PC2', data=pca_df2, hue=pca_df2.index, s=50)\n",
    "\n",
    "# texts = []\n",
    "# for i, language in enumerate(pca_df2.index):\n",
    "#     texts.append(plt.annotate(language, (pca_df2['PC1'][i], pca_df2['PC2'][i])))\n",
    "    \n",
    "# adjust_text(texts)\n",
    "\n",
    "# lang_labels = {\n",
    "#     'sv': 'Swedish', \n",
    "#     'id': 'Indonesian', \n",
    "#     'eo': 'Esperanto',\n",
    "#     'is': 'Icelandic',\n",
    "#     'pl': 'Polish',\n",
    "#     'da': 'Danish',\n",
    "#     'tl': 'Tagalog',\n",
    "#     'ia': 'Interlingua',\n",
    "#     'fi': 'Finnish',\n",
    "#     'vi': 'Vietnamese',\n",
    "#     'hu': 'Hungarian',\n",
    "#     'nl': 'Dutch',\n",
    "#     'lfn': 'Lingua Franca Nova',\n",
    "#     'es': 'Spanish',\n",
    "#     'io': 'Ido',\n",
    "#     'af': 'Afrikaans',\n",
    "#     'fr': 'French',\n",
    "#     'it': 'Italian',\n",
    "#     'tr': 'Turkish',\n",
    "#     'de': 'German',\n",
    "#     'oc': 'Occitan',\n",
    "#     'en': 'English',\n",
    "#     'vo': 'Volapük',\n",
    "#     'avk': 'Kotava'\n",
    "#     }\n",
    "# legend_labels = [lang_labels[abb] for abb in pca_df2.index]\n",
    "    \n",
    "# plt.xlabel('PC1')\n",
    "# plt.ylabel('PC2')\n",
    "# plt.title('PCA Visualization of Language Metrics, extended')\n",
    "# plt.grid(True)\n",
    "# plt.legend(title='Language', labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# # plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological Complexity (Morfessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing...\n",
    "segmentations = open('./segmodels/en_model.segm', 'r', encoding='utf-8').read().splitlines()[1:]\n",
    "segmentations = [line.lstrip('1').strip() for line in segmentations]\n",
    "print(segmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "segmodels_dir = '/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/segmodels'\n",
    "segmodels = [os.path.join(segmodels_dir, file) for file in os.listdir(segmodels_dir) if file.endswith('.segm')]\n",
    "\n",
    "# Average number of forms per lemma\n",
    "# lemmas_forms_dict = defaultdict(list)\n",
    "# for word in segmentations:\n",
    "#     segments = word.split(' + ')\n",
    "#     lemmas_forms_dict[max(segments, key=len)].extend([m for m in segments if m != max(segments, key=len)])\n",
    "\n",
    "# for i, (k, v) in enumerate(lemmas_forms_dict.items()):\n",
    "#     if i == 5:  \n",
    "#         break\n",
    "#     print(f\"{k}: {v}\")\n",
    "\n",
    "# Morpheme TTR\n",
    "# types = set([word.split(' + ') for word in segmentations])\n",
    "# types = {s for seg in [word.split(' + ') for word in segmentations] for s in seg if len(s) != len(max(seg, key=len))}\n",
    "# print(types)\n",
    "\n",
    "for file in segmodels:\n",
    "    data = open(file, 'r', encoding='utf-8').read().splitlines()[1:]\n",
    "    segmentations = [line.lstrip('1').strip() for line in data]\n",
    "    # segmentations = [line.split(' + ') for line in segmentations]\n",
    "    print(segmentations[:2])\n",
    "    # lang = os.path.splitext(os.path.basename(file))[0].split('_')[0]\n",
    "    # segmentations = [line.split(' + ') for line in segmentations]\n",
    "    # morphemes = [n[1:] for n in segmentations if n[1:]]\n",
    "    # lemmas = [n[0] for n in segmentations]\n",
    "    # lemmas_and_segmentations_dict = defaultdict(list)\n",
    "    # for seg in segmentations:\n",
    "    #     lemma = seg[0]\n",
    "    #     if seg[1:]:\n",
    "    #         lemmas_and_segmentations_dict[lemma].append(seg[1:])\n",
    "    # lemmas_and_segmentations = [(lemma, segmentations) for lemma, segmentations in lemmas_and_segmentations_dict.items()]\n",
    "    # n_morphs = len(set(morphemes))\n",
    "    # avg_seg_per_word = sum(len(forms) for forms in segmentations) / len(segmentations)\n",
    "    # n_forms_per_lemma = {i:lemmas.count(i) for i in set(lemmas)}\n",
    "    # lemma_prob_dist = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations = [line.split(' + ') for line in text]\n",
    "lemmas = [n[0] for n in segmentations]\n",
    "forms_per_lemma = {i:lemmas.count(i) for i in set(lemmas)}\n",
    "lemmas_and_segmentations_dict = defaultdict(list)\n",
    "for seg in segmentations:\n",
    "    lemma = seg[0]\n",
    "    if seg[1:]:\n",
    "        lemmas_and_segmentations_dict[lemma].append(seg[1:])\n",
    "lemmas_and_segmentations = [(lemma, segmentations) for lemma, segmentations in lemmas_and_segmentations_dict.items()]\n",
    "segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = {}\n",
    "morph_df = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('./segmodels'):\n",
    "    text = open(file, 'r', encoding='utf-8').read().splitlines()[1:]\n",
    "    text = [line.lstrip('1').strip() for line in text]\n",
    "    language = os.path.splitext(os.path.basename(file))[0].split('_')[0]\n",
    "    segmentations = [line.split(' + ') for line in text]\n",
    "    morphemes = [n[1:] for n in segmentations if n[1:]]\n",
    "    lemmas = [n[0] for n in segmentations]\n",
    "    lemmas_and_segmentations_dict = defaultdict(list)\n",
    "    for seg in segmentations:\n",
    "        lemma = seg[0]\n",
    "        if seg[1:]:\n",
    "            lemmas_and_segmentations_dict[lemma].append(seg[1:])\n",
    "    lemmas_and_segmentations = [(lemma, segmentations) for lemma, segmentations in lemmas_and_segmentations_dict.items()]\n",
    "    n_morphs = len(set(morphemes))\n",
    "    avg_seg_per_word = sum(len(forms) for forms in segmentations) / len(segmentations)\n",
    "    n_forms_per_lemma = {i:lemmas.count(i) for i in set(lemmas)}\n",
    "    lemma_prob_dist = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency Distribution (Char and Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, torch.nn as nn, re, random, pandas as pd, numpy as np, seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpora = ['Esperanto.txt', 'Interlingua.txt', 'Lojban.txt', 'Lfn.txt', 'Russian.txt', 'English.txt', 'German.txt', 'Japanese.txt', 'Mandarin.txt', 'Hindi.txt']\n",
    "# natural = ['Russian.txt', 'German.txt', 'English.txt', 'Japanese.txt', 'Mandarin.txt', 'Hindi.txt']\n",
    "# constructed = ['Esperanto.txt', 'Interlingua.txt', 'Lojban.txt', 'Lfn.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/2024_corpora/'\n",
    "corpora = [os.path.join(dir, file) for file in os.listdir(dir) if file.endswith('.txt')]\n",
    "constructed = ['eo_wiki_cleaned.txt', 'lfn_wiki_cleaned.txt', 'ia_wiki_cleaned.txt', 'io_wiki_cleaned.txt', 'avk_wiki_cleaned.txt', 'vo_wiki_cleaned.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Frequency Distribution\n",
    "\n",
    "def char_frequency(corpus, sequences=None, n_gram=None):\n",
    "    with open(corpus) as f:\n",
    "        if sequences:\n",
    "            text = f.read().splitlines()[:int(sequences / len(corpora))]\n",
    "            print(f'{len(text)} lines of text from {corpus[:(len(corpus)-4)]} corpus.')\n",
    "        else:\n",
    "            text = f.read().splitlines()\n",
    "        f.close()\n",
    "    df = pd.DataFrame(columns=['Char', 'Act_Freq', 'Rel_Freq', 'Zipf_Freq', 'Norm_Freq'])\n",
    "    if n_gram == 'uni':\n",
    "        frequencies = Counter(char for line in text for char in line if char.split())\n",
    "    elif n_gram == 'bi':\n",
    "        frequencies = Counter(word[i:i+2] for line in text for word in line.split() for i in range(len(word)-1))\n",
    "    else:\n",
    "        frequencies = Counter(char for line in text for char in line if char.split())   \n",
    "    frequencies = frequencies.most_common()\n",
    "    top_frequency = frequencies[0][1]\n",
    "    relative_freq_sum = sum([1/i for i in range(1, len(frequencies)+1)])\n",
    "    for index, item in enumerate(frequencies, start=1):\n",
    "        relative_freq = 1/index\n",
    "        zipf_freq = top_frequency * (1/index)\n",
    "        normalized_freq = relative_freq / relative_freq_sum\n",
    "        df.loc[index] = [item[0], item[1], relative_freq, zipf_freq, normalized_freq]\n",
    "    return df\n",
    "\n",
    "\n",
    "def char_vocabulary(corpus=None, df=None, sequences=None, n_gram=None):\n",
    "    if corpus:\n",
    "        df = char_frequency(corpus, sequences=sequences, n_gram=n_gram)\n",
    "    return {char: freq for char, freq in zip(df['Char'], df['Act_Freq'])}\n",
    "\n",
    "\n",
    "def plot_char_dist(df, chars=None):\n",
    "    if chars: df = df.head(chars)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.ylabel('Zipf Frequency')\n",
    "    plt.xlabel('Char')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.bar(df['Char'], df['Zipf_Freq'])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def corpus_lengths(corpora):\n",
    "    lengths = {}\n",
    "    for file in corpora:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = f.read().splitlines()\n",
    "            f.close()\n",
    "        lengths[f'{file.split(\".\")[0]}'] = len(data)\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def vocab_sizes(corpora):\n",
    "    sizes = {}\n",
    "    for file in corpora:\n",
    "        vocab = char_vocabulary(corpus=file, df=None, limit=None)\n",
    "        sizes[f'{file.split(\".\")[0]}'] = len(vocab)\n",
    "    return sizes\n",
    "    \n",
    "    \n",
    "def obfuscate_label_text(corpus, vocab, sequences=None, classification=None):\n",
    "    with open(corpus, 'r', encoding='utf-8') as f:\n",
    "        if sequences:\n",
    "            text = f.read().splitlines()[:int(sequences / len(corpora))]\n",
    "        else:\n",
    "            text = f.read().splitlines()\n",
    "        f.close()\n",
    "        # Char-based frequency mapping of characters in each string    \n",
    "        mapping = {value: chr(97 + i) for i, value in enumerate(vocab.values())}\n",
    "        vocab = {k: mapping[v] for k, v in vocab.items()}\n",
    "        table = str.maketrans(vocab)\n",
    "        # Labeling for binary classification\n",
    "        if classification:\n",
    "            if classification == 'binary':\n",
    "                if corpus in constructed:\n",
    "                    label = [0] * len(text)\n",
    "                else:\n",
    "                    label = [1] * len(text)\n",
    "            # Labeling for multi-classification\n",
    "            elif classification == 'multi':\n",
    "                label = [corpora.index(corpus)] * len(text)\n",
    "            text = list(zip(text, label))\n",
    "            obfuscated = [(line[0].translate(table), line[1]) for line in text]\n",
    "        obfuscated = [line.translate(table) for line in text]\n",
    "        f.close()\n",
    "    return obfuscated\n",
    "\n",
    "\n",
    "# def one_hot_encode(data, vocab):\n",
    "#     seq_length = max(len(line[0] for line in data))\n",
    "#     string_encoded = np.zeros((seq_length, len(vocab)), dtype=np.float32)\n",
    "#     for string, label in data:\n",
    "#         for i, char in enumerate(string):\n",
    "#             if i >= seq_length:\n",
    "#                 break\n",
    "#             string_encoded[i][vocab[char]] = 1\n",
    "#         label_encoded = np.array([label], dtype=np.int64)\n",
    "#     return string_encoded, label_encoded\n",
    "\n",
    "\n",
    "def preprocess_text(corpora, sequences=None, classification=None, n_gram=None):\n",
    "    data = []\n",
    "    for corpus in corpora:\n",
    "        df = char_frequency(corpus=corpus, sequences=sequences, n_gram=None)\n",
    "        vocab = char_vocabulary(df=df)\n",
    "        data.extend(obfuscate_label_text(corpus=corpus, vocab=vocab, sequences=sequences, classification=classification))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = char_frequency('../current_corpora/de_wiki_cleaned.txt', n_gram='bi')\n",
    "df = df.head(20)\n",
    "plot_char_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df['Act_Freq'].cumsum() / df['Act_Freq'].sum()\n",
    "a = 1.5\n",
    "null_hypothesis = 'zipf'\n",
    "zipf_dist = stats.zipf(a, loc=1)\n",
    "zipf_cdf = zipf_dist.cdf(range(1, len(df) + 1))\n",
    "test_statistic, p_value = stats.kstest(cdf, zipf_cdf)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char Embeddings: Char2Vec (from https://github.com/sonlamho/Char2Vec/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Char2Vec.char2vec.utils import *\n",
    "from Char2Vec.char2vec.embed import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = '/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/current_corpora/en_wiki_extractor.txt'\n",
    "alpha = 'abcdefghijklmnopqrstuvwxyz '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Char2Vec(corpus_path, config=CONFIG, alphabet=alpha, unk='~')\n",
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Char Embeddings\n",
    "def scatter2d_with_annotation(arr, text, xcol=0, ycol=1, fontsize=7, show=False, **kwargs):\n",
    "    \n",
    "    assert len(arr)==len(text)\n",
    "    fig, ax = plt.subplots(**kwargs)\n",
    "    xarr = arr[:, xcol]\n",
    "    yarr = arr[:, ycol]\n",
    "    plt_texts = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        plt.scatter(xarr[i], yarr[i])\n",
    "        plt_texts.append(plt.text(xarr[i], yarr[i], text[i], fontsize=fontsize))\n",
    "        \n",
    "    if show:\n",
    "        plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "# Char embedding matrix U\n",
    "U = m.U_\n",
    "\n",
    "p = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U2 = p.fit_transform(U)\n",
    "texts = m._tokenizer.alphabet + [m._tokenizer.unk]\n",
    "fig, ax = scatter2d_with_annotation(\n",
    "            U2, [s.upper() for s in texts],\n",
    "            0,1,\n",
    "            figsize=(8,5), fontsize=13)\n",
    "plt.title('PCA of Character embeddings, Dim-0 vs Dim-1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U3 = TSNE(n_components=2, perplexity=5).fit_transform(U)\n",
    "fig, ax = scatter2d_with_annotation(\n",
    "            U3, [s.upper() for s in texts],\n",
    "            0,1,\n",
    "            figsize=(8,5), fontsize=13)\n",
    "plt.title('T-SNE of Character embeddings, Perplexity=5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
