{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 13:45:19.485089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import chain\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text = open('/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/current_corpora/en_wiki_extractor.txt', 'r', encoding='utf-8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabet in language\n",
    "chars = list(set(chain(*(char for line in text for char in line if line if not char.isspace()))))\n",
    "# Unique set of tokens in text\n",
    "tokens = list(set(chain(*(line.split() for line in text if line))))\n",
    "# BOS char\n",
    "chars.insert(0, '<')\n",
    "# EOS char\n",
    "chars.insert(1, '>')\n",
    "vocab_size = len(chars)\n",
    "max_len = len(max(tokens, key=len))\n",
    "\n",
    "# Make into sequences\n",
    "ch2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2ch = {i: c for i, c in enumerate(chars)}\n",
    "sequences = [chars[0] + t + chars[1] for t in tokens]\n",
    "\n",
    "# Convert to tensors, add padding, split into X_train/y_train and test set\n",
    "X = pad_sequences([[ch2idx[i] for i in seq[:-1]] for seq in sequences])\n",
    "y = pad_sequences([[ch2idx[i] for i in seq[1:]] for seq in sequences])\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 128)         3584      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 128)         131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 28)          3612      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138780 (542.11 KB)\n",
      "Trainable params: 138780 (542.11 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "loss_function = SparseCategoricalCrossentropy()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "optimizer = Adam()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, callbacks=[callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_perplexity(model, test_data):\n",
    "#     test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "#     test_sequences = np.array(test_sequences)\n",
    "#     test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_len - 1, padding='pre')\n",
    "#     total_log_loss = 0\n",
    "#     total_words = 0\n",
    "\n",
    "#     for sequence in test_sequences:\n",
    "#         input_seq = sequence[:-1]\n",
    "#         target = sequence[-1]\n",
    "\n",
    "#         predicted = model.predict(input_seq, verbose=0)[0]\n",
    "#         if target != 0:\n",
    "#             total_log_loss += np.log(predicted[int(target)])\n",
    "#             total_words += 1\n",
    "\n",
    "#     avg_log_loss = total_log_loss / total_words\n",
    "#     perplexity = np.exp(-avg_log_loss)\n",
    "#     return perplexity\n",
    "\n",
    "# # Test data for calculating perplexity\n",
    "# test_data = [\"Finally, here is the third sentence.\", \"This is the first sentence.\"]\n",
    "# perplexity = calc_perplexity(model, test_data)\n",
    "# print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
