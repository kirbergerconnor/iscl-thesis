{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 14:51:56.282621: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import chain\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/current_corpora/en_wiki_extractor.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "text = [line.translate(str.maketrans(\"\", \"\", string.punctuation)) for line in text]\n",
    "text = [line.translate(str.maketrans(\"\", \"\", string.digits)) for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(chain(*(line.split() for line in text if line))))\n",
    "vocab.insert(0, '<BOS>')\n",
    "vocab.insert(1, '<EOS>')\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2wd = {i: word for i, word in enumerate(vocab)}\n",
    "sequences = []\n",
    "\n",
    "for i in range(2, len(vocab)):\n",
    "    seq = [vocab[0], vocab[i], vocab[1]]\n",
    "    sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<BOS>', 'rainfall', '<EOS>'],\n",
       " ['<BOS>', 'reorganisation', '<EOS>'],\n",
       " ['<BOS>', 'require', '<EOS>'],\n",
       " ['<BOS>', 'florins', '<EOS>'],\n",
       " ['<BOS>', 'sacaba', '<EOS>'],\n",
       " ['<BOS>', 'relay', '<EOS>'],\n",
       " ['<BOS>', 'nebulosity', '<EOS>'],\n",
       " ['<BOS>', 'interspersed', '<EOS>'],\n",
       " ['<BOS>', 'inuyasha', '<EOS>'],\n",
       " ['<BOS>', 'unconvincingly', '<EOS>']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[wd2idx[word] for word in sequence[:-1]] for sequence in sequences]\n",
    "y = [[wd2idx[word] for word in sequence[1:]] for sequence in sequences]\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([33827,     2], dtype=int32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int32, numpy=array([33827,     2], dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(X_train), tf.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = np.asarray(sequences)[:, np.newaxis]\n",
    "# X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "# seq_len = X.shape[1]\n",
    "\n",
    "# max_len = len(max(text, key=lambda x: len(x.split())).split())\n",
    "\n",
    "# predictors, label = input_seqs[:, :-1], input_seqs[:, -1]\n",
    "# label = tf.keras.utils.to_categorical(label, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(text)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "# sequences = tokenizer.texts_to_sequences(text)\n",
    "# for line in text:\n",
    "#     token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "#     for i in range(1, len(token_list)):\n",
    "#         seq = token_list[i - 1:i + 1]\n",
    "#         input_seqs.append(seq)\n",
    "        \n",
    "# max_token_len = max([len(x) for x in sequences])\n",
    "# sequences = pad_sequences(sequences, maxlen=max_token_len, padding='post')\n",
    "# # input_seqs = np.array(input_seqs)\n",
    "# X, y = sequences[:, :-1], sequences[:, -1]\n",
    "# y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# split_index = int(0.8 * len(X))\n",
    "# X_train, X_test = X[:split_index], X[split_index:]\n",
    "# y_train, y_test = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         5412608   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 42286)             5454894   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10999086 (41.96 MB)\n",
      "Trainable params: 10999086 (41.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "loss_function = SparseCategoricalCrossentropy()\n",
    "optimizer = Adam()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=None))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 728, in start\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 215, in start\n\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 423, in do_execute\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n\n  File \"/var/folders/1w/_90xn6_52fq9d37y9_7snxr40000gn/T/ipykernel_3889/4148055642.py\", line 1, in <module>\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [32,42286] and labels shape [64]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_3342]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/kerasrnn.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/kerasrnn.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49mnum_epochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 728, in start\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 215, in start\n\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n\n  File \"/usr/local/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 423, in do_execute\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n\n  File \"/Users/k/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n\n  File \"/var/folders/1w/_90xn6_52fq9d37y9_7snxr40000gn/T/ipykernel_3889/4148055642.py\", line 1, in <module>\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/usr/local/lib/python3.9/site-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [32,42286] and labels shape [64]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_3342]"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_perplexity(model, test_data):\n",
    "#     test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "#     test_sequences = np.array(test_sequences)\n",
    "#     test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_len - 1, padding='pre')\n",
    "#     total_log_loss = 0\n",
    "#     total_words = 0\n",
    "\n",
    "#     for sequence in test_sequences:\n",
    "#         input_seq = sequence[:-1]\n",
    "#         target = sequence[-1]\n",
    "\n",
    "#         predicted = model.predict(input_seq, verbose=0)[0]\n",
    "#         if target != 0:\n",
    "#             total_log_loss += np.log(predicted[int(target)])\n",
    "#             total_words += 1\n",
    "\n",
    "#     avg_log_loss = total_log_loss / total_words\n",
    "#     perplexity = np.exp(-avg_log_loss)\n",
    "#     return perplexity\n",
    "\n",
    "# # Test data for calculating perplexity\n",
    "# test_data = [\"Finally, here is the third sentence.\", \"This is the first sentence.\"]\n",
    "# perplexity = calc_perplexity(model, test_data)\n",
    "# print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
