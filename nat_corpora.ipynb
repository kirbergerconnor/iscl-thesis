{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "\n",
    "with open('./Languages/english/eng_news_2020_100K-sentences.txt', 'r', encoding='utf-8') as en_f:\n",
    "    text = en_f.readlines()\n",
    "    en_f.close()\n",
    "sentences = [line.lower().split('\\t')[1] for line in text]\n",
    "alpha = re.compile(r'[^a-z\\s]+')\n",
    "en_filtered = [' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence]\n",
    "\n",
    "with open('English.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(en_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Russian\n",
    "\n",
    "with open('./Languages/russian/rus_news_2021_100K-sentences.txt', 'r', encoding='utf-8') as ru_f:\n",
    "    text = ru_f.readlines()\n",
    "    ru_f.close()\n",
    "sentences = [line.lower().split('\\t')[1] for line in text]\n",
    "alpha = re.compile(r'[^\\u0400-\\u04FF\\s]+')\n",
    "ru_filtered = [' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence]\n",
    "\n",
    "with open('Russian.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(ru_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arabic\n",
    "\n",
    "# with open('./Languages/arabic/ara_news_2020_100K-sentences.txt', 'r', encoding='utf-8') as ar_f:\n",
    "#     text = ar_f.readlines()\n",
    "#     ar_f.close()\n",
    "# sentences = [line.lower().split('\\t')[1] for line in text]\n",
    "# alpha = re.compile(r'[^\\u0621-\\u0626\\s]+')\n",
    "# # ar_filtered = [re.sub(alpha, ' ', sentence) for sentence in sentences]\n",
    "# lines = [word for word in sentences]\n",
    "# print(lines[:20])\n",
    "\n",
    "# with open('Arabic.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write('\\n'.join(ar_filtered))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mandarin Chinese\n",
    "\n",
    "with open('./Languages/mandarin/cmn_wikipedia_2021_100K-sentences.txt', 'r', encoding='utf-8') as ch_f:\n",
    "    text = ch_f.readlines()\n",
    "    ch_f.close()\n",
    "sentences = [line.lower().split('\\t')[1] for line in text]\n",
    "alpha = re.compile(r'[^\\u4E00-\\u9FFF\\s]+')\n",
    "ch_filtered = [' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence]\n",
    "\n",
    "with open('Mandarin.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(ch_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German\n",
    "\n",
    "with open('./Languages/german/deu_news_2021_100K-sentences.txt', 'r', encoding='utf-8') as de_f:\n",
    "    text = de_f.readlines()\n",
    "    de_f.close()\n",
    "sentences = [line.lower().split('\\t')[1] for line in text]\n",
    "alpha = re.compile(r'[^a-z\\u00DF\\u00E4\\u00F6\\u00FC\\s]+')\n",
    "de_filtered = [' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence]\n",
    "\n",
    "with open('German.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(de_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japanese\n",
    "\n",
    "with open('./Languages/japanese/jpn_news_2020_100K-sentences.txt', 'r', encoding='utf-8') as jp_f:\n",
    "    text = jp_f.readlines()\n",
    "    jp_f.close()\n",
    "sentences = [line.lower().split('\\t')[1] for line in text]\n",
    "alpha = re.compile(r'[^\\u3040-\\u309f\\u30a0-\\u30ff\\u4e00-\\u9faf\\u3400-\\u4dbf\\s]+')\n",
    "jp_filtered = [' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence]\n",
    "\n",
    "with open('Japanese.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(jp_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindi\n",
    "\n",
    "with open('./Languages/hindi/hin_news_2020_100K-sentences.txt', 'r', encoding='utf-8') as hi_f:\n",
    "    text = hi_f.readlines()\n",
    "    hi_f.close()\n",
    "sentences = [line.lower().split('\\t')[1] for line in text]\n",
    "alpha = re.compile(r'[^\\u0900-\\u0965\\u0970-\\u097F\\s]+')\n",
    "hi_filtered = [' '.join(re.sub(alpha, ' ', sentence).split()) for sentence in sentences if sentence]\n",
    "\n",
    "with open('Hindi.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(hi_filtered))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
