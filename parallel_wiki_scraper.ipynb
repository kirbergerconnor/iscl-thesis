{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['eo', 'ia', 'lfn', 'en', 'de', 'fr']\n",
    "page_titles = ['United_Kingdom', 'Japan', 'South_Korea', 'Brazil', 'North_Korea', 'Hong_Kong']\n",
    "\n",
    "# Use with specific urls, otherwise hardcode page_titles\n",
    "# list_url = \"https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_articles_written_in_the_greatest_number_of_languages\"\n",
    "# site = requests.get(list_url)\n",
    "# soup = BeautifulSoup(site.content, \"html.parser\")\n",
    "# for p in soup.find_all('p'):\n",
    "#     for a in p.find_all('a'):\n",
    "#         page_titles.append(a.get('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia_pages(languages, page_titles):\n",
    "    corpus = {}\n",
    "    for title in page_titles:\n",
    "        exists_in_all_languages = True\n",
    "        for lang in languages:\n",
    "            wiki_lang = wikipediaapi.Wikipedia(language=lang)\n",
    "            page = wiki_lang.page(title)\n",
    "            if not page.exists():\n",
    "                exists_in_all_languages = False\n",
    "                break\n",
    "\n",
    "        if exists_in_all_languages:\n",
    "            corpus[title] = {}\n",
    "            for lang in languages:\n",
    "                wiki_lang = wikipediaapi.Wikipedia(language=lang, extract_format=wikipediaapi.ExtractFormat.HTML)\n",
    "                page = wiki_lang.page(title)\n",
    "                p_soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "                # div = p_soup.find('div', attrs={'class': 'mw-body-content'})\n",
    "                p_tag = p_soup.find_all('p')\n",
    "                text = ' '.join(p.get_text() for p in p_tag if p.get_text().strip())\n",
    "                corpus[title][lang] = text\n",
    "                \n",
    "    return corpus\n",
    "\n",
    "def save_corpus_to_files(corpus):\n",
    "    for title, data in corpus.items():\n",
    "        for lang, content in data.items():\n",
    "            filename = f'{title}_{lang}.txt'\n",
    "            with open(f'./parallel/{filename}', 'w', encoding='utf-8') as file:\n",
    "                file.write(content)\n",
    "\n",
    "\n",
    "corpus = scrape_wikipedia_pages(languages, page_titles)\n",
    "\n",
    "save_corpus_to_files(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
