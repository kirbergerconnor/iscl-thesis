{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk, math, glob, re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts_by_language(folder_path, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        language = file_name.split('_')[-1].split('.')[0]\n",
    "\n",
    "        try:\n",
    "            with open(file_path, encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                pass\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Error opening file: {file_name}\")\n",
    "\n",
    "        output_file = os.path.join(output_folder, f\"{language}.txt\")\n",
    "        with open(output_file, 'a', encoding='utf-8') as output:\n",
    "            output.write(text + '\\n')\n",
    "\n",
    "folder_path = '/Users/k/Dev/Webscraping/build-source-text/parallel'\n",
    "output_folder = '/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/'\n",
    "\n",
    "extract_texts_by_language(folder_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance Corpus Data\n",
    "\n",
    "directory = '/Users/k/Dev/Webscraping/build-source-text/parallel2'\n",
    "output_dir = '/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/Balanced/'\n",
    "\n",
    "files = glob.glob(os.path.join(directory, '*.txt'))\n",
    "topics = {}\n",
    "\n",
    "def shorten_text(text, target_wc):\n",
    "    words = text.split()\n",
    "    if len(words) <= target_wc:\n",
    "        return text\n",
    "    \n",
    "    sentences = text.split('. ')  \n",
    "    wc = 0\n",
    "    output_sen = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        sentence_wc = len(words)\n",
    "        if wc + sentence_wc <= target_wc:\n",
    "            output_sen.append(sentence)\n",
    "            wc += sentence_wc\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    shortened_text = '.'.join(output_sen)\n",
    "    return shortened_text\n",
    "\n",
    "for file in files:\n",
    "    topic = os.path.basename(file).split('_')[0]\n",
    "    if topic in topics:\n",
    "        topics[topic].append(file)\n",
    "    else:\n",
    "        topics[topic] = [file]\n",
    "        \n",
    "for topic, files in topics.items():\n",
    "    wc_min = float('inf')\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            wc = len(f.read().split())\n",
    "            \n",
    "        if wc < wc_min:\n",
    "            wc_min = wc\n",
    "        f.close()\n",
    "        \n",
    "    for file in files:\n",
    "        language = file.split('_')[-1].split('.')[0]\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            shortened_text = shorten_text(text, wc_min)\n",
    "            f.close()\n",
    "        output_file = os.path.join(output_dir, f\"{language}.txt\")\n",
    "        with open(output_file, 'a', encoding='utf-8') as output:\n",
    "            output.write(shortened_text + '\\n')        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def calculate_word_probabilities(tokens):\n",
    "    word_counts = Counter(tokens)\n",
    "    total_words = len(tokens)\n",
    "    word_probabilities = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return word_probabilities\n",
    "\n",
    "def calculate_entropy(word_probabilities):\n",
    "    entropy = 0\n",
    "    for word, probability in word_probabilities.items():\n",
    "        entropy += probability * math.log2(probability)\n",
    "    entropy = -entropy\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/de.txt', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "    tokens = preprocess_text(corpus_text)\n",
    "    word_probabilities = calculate_word_probabilities(tokens)\n",
    "    entropy = calculate_entropy(word_probabilities)\n",
    "    \n",
    "    print(\"Entropy: \", entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/'\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith('.txt'):  \n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            corpus_text = f.read()\n",
    "            tokens = preprocess_text(corpus_text)\n",
    "            word_probabilities = calculate_word_probabilities(tokens)\n",
    "            entropy = calculate_entropy(word_probabilities)\n",
    "\n",
    "            print(f'Entropy of {f}: ', entropy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/en.txt') as f:\n",
    "    text = f.read()\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    word_lengths = [len(word) for word in tokens]\n",
    "    frequency_distribution = nltk.FreqDist(word_lengths)\n",
    "    print(frequency_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def calculate_word_lengths(tokens):\n",
    "    word_lengths = [len(word) for word in tokens]\n",
    "    fdist = nltk.FreqDist(word_lengths)\n",
    "    return fdist\n",
    "\n",
    "def plot_word_length_distribution(frequency_distribution):\n",
    "    plt.bar(frequency_distribution.keys(), frequency_distribution.values())\n",
    "    plt.xlabel('Word Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Word Length Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/en.txt', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "    tokens = preprocess_text(corpus_text)\n",
    "    fdist = calculate_word_lengths(tokens)\n",
    "    plot_word_length_distribution(fdist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/lfn.txt', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "    tokens = preprocess_text(corpus_text)\n",
    "    fdist = calculate_word_lengths(tokens)\n",
    "    fdist.plot(title='Word Length Dist', cumulative=False, percents=False, show=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def calculate_ttr(tokens):\n",
    "    unique_words = set(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    ttr = len(unique_words) / total_tokens\n",
    "    return ttr\n",
    "\n",
    "with open('/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/en.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    tokens = preprocess_text(text)\n",
    "    ttr = calculate_ttr(tokens)\n",
    "    print(\"Type-Token Ratio:\", ttr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexicalrichness import LexicalRichness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8761885656971639"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/Balanced/en.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "lex = LexicalRichness(text)\n",
    "lex.mattr(window_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8082704607754454"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/Parallel/Balanced/lfn.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "lex = LexicalRichness(text)\n",
    "lex.mattr(window_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
