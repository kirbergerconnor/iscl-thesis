{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and preprocess text data for each language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using /iscl-thesis/WikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, string\n",
    "\n",
    "def clean_sentences(content):\n",
    "    # Remove page titles\n",
    "    no_titles = re.sub(r'(?<=\\s\\n).+:\\n', '', content)\n",
    "    # Remove section headers\n",
    "    no_headers = re.sub(r'.+\\/\\/\\/\\/\\/\\n', '', no_titles)\n",
    "    # Remove picture links\n",
    "    no_pics = re.sub(r'(?<=\\n)(\\w+\\|)+.+[^\\.]\\n', '', no_headers)\n",
    "    # Remove fragments\n",
    "    no_fragments = re.sub(r'(?<=\\n)\\w+\\:\\w+.*[^\\.](?=\\n)', '', no_pics)\n",
    "    # Remove other links\n",
    "    no_links = re.sub(r'http[^\\s\\n]+(?=\\s|\\n)', '', no_fragments)\n",
    "    # Remove paranthesis\n",
    "    no_paranthesis = re.sub(r'\\([^\\)]+\\)', '', no_links)\n",
    "    # Removes braces\n",
    "    no_braces = re.sub(r'\\[+\\w+[^\\]]*\\]+', '', no_paranthesis)\n",
    "    # Remove lists\n",
    "    no_lists = re.sub(r'(?<=\\n)[^\\.\\n]+\\:\\s?\\n', '', no_braces)\n",
    "    # Remove html tags\n",
    "    no_html = re.sub(r'\\<[^\\>]+\\>', '', no_lists)\n",
    "    # Remove number fragments\n",
    "    no_num = re.sub(r'\\d+(?=\\n)', '', no_html)\n",
    "    # Replacing hyphens, dashes, and forward slashes in compound words with spaces\n",
    "    no_hyph = re.sub(r'[\\-\\‐\\‑\\‒\\–\\—\\―\\⁃\\−\\/]', ' ', no_num)\n",
    "    # Split at periods while trying to overlook abbreviations\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w)(?<!\\w\\.)(?<!Sr)(?<!Mr)(?<!Ms)(?<!Mrs)(?<!Jr)(?<!Dr)\\.(?!\\w)', no_hyph)\n",
    "    # Remove punctuation, numbers, make lowercase\n",
    "    return [s.translate(str.maketrans('', '', string.punctuation + string.digits)).lower() for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22 tl corpus finished. 649999 words. 30748 sentences.\n",
      "2/22 oc corpus finished. 649976 words. 34847 sentences.\n",
      "3/22 af corpus finished. 649985 words. 31840 sentences.\n",
      "4/22 eo corpus finished. 649988 words. 34620 sentences.\n",
      "5/22 en corpus finished. 649975 words. 30333 sentences.\n",
      "6/22 tr corpus finished. 649990 words. 45329 sentences.\n",
      "7/22 io corpus finished. 649994 words. 45076 sentences.\n",
      "8/22 de corpus finished. 649989 words. 38738 sentences.\n",
      "9/22 fr corpus finished. 649993 words. 28319 sentences.\n",
      "10/22 id corpus finished. 649989 words. 35504 sentences.\n",
      "11/22 vi corpus finished. 649981 words. 21795 sentences.\n",
      "12/22 sv corpus finished. 649998 words. 38051 sentences.\n",
      "13/22 ia corpus finished. 649999 words. 33181 sentences.\n",
      "14/22 nl corpus finished. 649995 words. 35800 sentences.\n",
      "15/22 es corpus finished. 649983 words. 25724 sentences.\n",
      "16/22 da corpus finished. 649981 words. 39197 sentences.\n",
      "17/22 it corpus finished. 649994 words. 25326 sentences.\n",
      "18/22 pl corpus finished. 649991 words. 43463 sentences.\n",
      "19/22 lfn corpus finished. 628214 words. 32128 sentences.\n",
      "20/22 fi corpus finished. 649999 words. 53751 sentences.\n",
      "21/22 is corpus finished. 649997 words. 43337 sentences.\n",
      "22/22 hu corpus finished. 649993 words. 41290 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Directory with extracted files\n",
    "maindir = '/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/WikiExtractorCorpora/'\n",
    "# Dict of languages + their corresponding alphabets\n",
    "alphabets = {\n",
    "    # Lingua Franca Nova\n",
    "    'lfn': 'abcdefghijklmnopqrstuvwxyz', \n",
    "    # Interlingua\n",
    "    'ia': 'abcdefghijklmnopqrstuvwxyz', \n",
    "    # Esperanto\n",
    "    'eo': 'abcĉdefgĝhĥijĵklmnoprsŝtuŭvz', \n",
    "    # German\n",
    "    'de': 'abcdefghijklmnopqrstuvwxyzäöüß', \n",
    "    # French\n",
    "    'fr': 'abcdefghijklmnopqrstuvwxyzéàèùëüïâêîôûçæœ', \n",
    "    # English\n",
    "    'en': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Finnish\n",
    "    'fi': 'abcdefghijklmnopqrstuvwxyzšžåäö',\n",
    "    # Tagalog\n",
    "    'tl': 'abcdefghijklmnopqrstuvwxyzñ',\n",
    "    # Turkish\n",
    "    'tr': 'abcçdefgğhıijklmnoöprsştuüvyz',\n",
    "    # Vietnamese\n",
    "    'vi': 'aáàảãạăắằẳẵặâấầẩẫậbcdđeéèẻẽẹêếềểễệfghiíìỉĩịjklmnoóòỏõọôốồổỗộơớờởỡợpqrstuúùủũụưứừửữựvwxyýỳỷỹỵz',\n",
    "    # Polish\n",
    "    'pl': 'aąbcćdeęfghijklłmnńoópqrsśtuvwxyzźż',\n",
    "    # Indonesian\n",
    "    'id': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Ido\n",
    "    'io': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Italian\n",
    "    'it': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Dutch\n",
    "    'nl': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Occitan\n",
    "    'oc': 'abcdefghijklmnopqrstuvwxyzàèòáéíóúïüç',\n",
    "    # Danish\n",
    "    'da': 'abcdefghijklmnopqrstuvwxyzæøå',\n",
    "    # Swedish\n",
    "    'sv': 'abcdefghijklmnopqrstuvwxyzåäö',\n",
    "    # Hungarian\n",
    "    'hu': 'aábccsddzdzseéfggyhiíjkllymnnyoóöőpqrsszttyuúüűvwxyzzs',\n",
    "    # Spanish\n",
    "    'es': 'abcdefghijklmnopqrstuvwxyzñ',\n",
    "    # Afrikaans\n",
    "    'af': 'aáäbcdeéèêëfghiíîïjklmnŉoóôöpqrstuúûüvwxyýz',\n",
    "    # Icelandic\n",
    "    'is': 'aábdðeéfghiíjklmnoóprstuúvxyýþæö',\n",
    "    }\n",
    "\n",
    "# LFN corpus is the smallest size based on word count, at about 650000 words, so shrink all other corpora to about the same\n",
    "max_wc = 650000\n",
    "file_count = 1\n",
    "\n",
    "for file in os.listdir(maindir):\n",
    "    if file.endswith('.txt'):\n",
    "        fullpath = os.path.join(maindir, file)\n",
    "        lang = os.path.splitext(os.path.basename(file))[0].split('_')[0]\n",
    "        current_wc = 0\n",
    "        if lang in alphabets:\n",
    "            alpha = set(alphabets[lang])\n",
    "        with open(f'/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/current_corpora/{lang}_wiki_cleaned.txt', 'w', encoding='utf-8') as out:\n",
    "            with open(fullpath, 'r', encoding='utf-8') as f: \n",
    "                content = f.read()\n",
    "                f.close()\n",
    "            sentences = clean_sentences(content)\n",
    "            sen_count = 0\n",
    "            for sentence in sentences:\n",
    "                # Remove all chars not language's alphabet\n",
    "                s = ''.join(char if char in alpha or char.isspace() else '' for char in sentence)\n",
    "                # Remove leftover whitespaces\n",
    "                s = re.sub(r'\\s+', ' ', s).strip()\n",
    "                # Remove sentence fragments containing only 1 word\n",
    "                if len(s.split()) > 1:\n",
    "                    if (len(s.split()) + current_wc) < max_wc:\n",
    "                        current_wc += len(s.split())\n",
    "                        out.write(s + '\\n')\n",
    "                        sen_count += 1\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        print(f'{file_count}/22 {lang} corpus finished. {current_wc} words. {sen_count} sentences.')\n",
    "        file_count += 1\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
