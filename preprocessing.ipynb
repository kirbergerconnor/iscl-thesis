{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and preprocess text data for each language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using /iscl-thesis/WikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, string\n",
    "\n",
    "def clean_sentences(content):\n",
    "    # Remove page titles\n",
    "    no_titles = re.sub(r'(?<=\\s\\n).+:\\n', '', content)\n",
    "    # Remove section headers\n",
    "    no_headers = re.sub(r'.+\\/\\/\\/\\/\\/\\n', '', no_titles)\n",
    "    # Remove picture links\n",
    "    no_pics = re.sub(r'(?<=\\n)(\\w+\\|)+.+[^\\.]\\n', '', no_headers)\n",
    "    # Remove fragments\n",
    "    no_fragments = re.sub(r'(?<=\\n)\\w+\\:\\w+.*[^\\.](?=\\n)', '', no_pics)\n",
    "    # Remove other links\n",
    "    no_links = re.sub(r'http[^\\s\\n]+(?=\\s|\\n)', '', no_fragments)\n",
    "    # Remove paranthesis\n",
    "    no_paranthesis = re.sub(r'\\([^\\)]+\\)', '', no_links)\n",
    "    # Removes braces\n",
    "    no_braces = re.sub(r'\\[+\\w+[^\\]]*\\]+', '', no_paranthesis)\n",
    "    # Remove lists\n",
    "    no_lists = re.sub(r'(?<=\\n)[^\\.\\n]+\\:\\s?\\n', '', no_braces)\n",
    "    # Remove html tags\n",
    "    no_html = re.sub(r'\\<[^\\>]+\\>', '', no_lists)\n",
    "    # Remove number fragments\n",
    "    no_num = re.sub(r'\\d+(?=\\n)', '', no_html)\n",
    "    # Replacing hyphens, dashes, and forward slashes in compound words with spaces\n",
    "    no_hyph = re.sub(r'[\\-\\‐\\‑\\‒\\–\\—\\―\\⁃\\−\\/]', ' ', no_num)\n",
    "    # Split at periods while trying to overlook abbreviations\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w)(?<!\\w\\.)(?<!Sr)(?<!Mr)(?<!Ms)(?<!Mrs)(?<!Jr)(?<!Dr)\\.(?!\\w)', no_hyph)\n",
    "    # Remove punctuation, numbers, make lowercase\n",
    "    return [s.translate(str.maketrans('', '', string.punctuation + string.digits)).lower() for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/24 is corpus finished. 629995 words. 41847 sentences.\n",
      "2/24 de corpus finished. 629987 words. 37261 sentences.\n",
      "3/24 pl corpus finished. 629997 words. 42138 sentences.\n",
      "4/24 io corpus finished. 629990 words. 43496 sentences.\n",
      "5/24 af corpus finished. 629994 words. 30737 sentences.\n",
      "6/24 avk corpus finished. 617400 words. 48145 sentences.\n",
      "7/24 hu corpus finished. 629946 words. 39916 sentences.\n",
      "8/24 lfn corpus finished. 628683 words. 32188 sentences.\n",
      "9/24 da corpus finished. 629999 words. 38260 sentences.\n",
      "10/24 es corpus finished. 629978 words. 24886 sentences.\n",
      "11/24 ia corpus finished. 629996 words. 32229 sentences.\n",
      "12/24 fr corpus finished. 629983 words. 27248 sentences.\n",
      "13/24 oc corpus finished. 629998 words. 33762 sentences.\n",
      "14/24 eo corpus finished. 629994 words. 33317 sentences.\n",
      "15/24 nl corpus finished. 629997 words. 34627 sentences.\n",
      "16/24 tr corpus finished. 629995 words. 43573 sentences.\n",
      "17/24 en corpus finished. 629958 words. 29574 sentences.\n",
      "18/24 tl corpus finished. 629989 words. 29855 sentences.\n",
      "19/24 sv corpus finished. 629998 words. 36370 sentences.\n",
      "20/24 vi corpus finished. 629958 words. 21115 sentences.\n",
      "21/24 it corpus finished. 629987 words. 24487 sentences.\n",
      "22/24 vo corpus finished. 629999 words. 55920 sentences.\n",
      "23/24 id corpus finished. 629997 words. 34683 sentences.\n",
      "24/24 fi corpus finished. 629994 words. 52637 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Directory with extracted files\n",
    "maindir = '/Users/k/Docs/School/Tuebingen/Thesis/Corpuses/NewWikiExtractorCorpora/'\n",
    "# Dict of languages + their corresponding alphabets\n",
    "alphabets = {\n",
    "    # Volapük\n",
    "    'vo': 'abcdefghijklmnoprstuvxyzöäü',\n",
    "    # Kotava\n",
    "    'avk': 'abcdefgijklmnoprstuvwxyzáéíóú',\n",
    "    # Lingua Franca Nova\n",
    "    'lfn': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Interlingua\n",
    "    'ia': 'abcdefghijklmnopqrstuvwxyz', \n",
    "    # Esperanto\n",
    "    'eo': 'abcĉdefgĝhĥijĵklmnoprsŝtuŭvz', \n",
    "    # German\n",
    "    'de': 'abcdefghijklmnopqrstuvwxyzäöüß', \n",
    "    # French\n",
    "    'fr': 'abcdefghijklmnopqrstuvwxyzéàèùëüïâêîôûçæœ', \n",
    "    # English\n",
    "    'en': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Finnish\n",
    "    'fi': 'abcdefghijklmnopqrstuvwxyzšžåäö',\n",
    "    # Tagalog\n",
    "    'tl': 'abcdefghijklmnopqrstuvwxyzñ',\n",
    "    # Turkish\n",
    "    'tr': 'abcçdefgğhıijklmnoöprsştuüvyz',\n",
    "    # Vietnamese\n",
    "    'vi': 'aáàảãạăắằẳẵặâấầẩẫậbcdđeéèẻẽẹêếềểễệfghiíìỉĩịjklmnoóòỏõọôốồổỗộơớờởỡợpqrstuúùủũụưứừửữựvwxyýỳỷỹỵz',\n",
    "    # Polish\n",
    "    'pl': 'aąbcćdeęfghijklłmnńoópqrsśtuvwxyzźż',\n",
    "    # Indonesian\n",
    "    'id': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Ido\n",
    "    'io': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Italian\n",
    "    'it': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Dutch\n",
    "    'nl': 'abcdefghijklmnopqrstuvwxyz',\n",
    "    # Occitan\n",
    "    'oc': 'abcdefghijklmnopqrstuvwxyzàèòáéíóúïüç',\n",
    "    # Danish\n",
    "    'da': 'abcdefghijklmnopqrstuvwxyzæøå',\n",
    "    # Swedish\n",
    "    'sv': 'abcdefghijklmnopqrstuvwxyzåäö',\n",
    "    # Hungarian\n",
    "    'hu': 'aábccsddzdzseéfggyhiíjkllymnnyoóöőpqrsszttyuúüűvwxyzzs',\n",
    "    # Spanish\n",
    "    'es': 'abcdefghijklmnopqrstuvwxyzñ',\n",
    "    # Afrikaans\n",
    "    'af': 'aáäbcdeéèêëfghiíîïjklmnŉoóôöpqrstuúûüvwxyýz',\n",
    "    # Icelandic\n",
    "    'is': 'aábdðeéfghiíjklmnoóprstuúvxyýþæö',\n",
    "    }\n",
    "\n",
    "# LFN corpus is the smallest size based on word count, at about 650000 words, so shrink all other corpora to about the same\n",
    "# Edit: now using LFN corpus with about 630000 words\n",
    "max_wc = 630000\n",
    "file_count = 1\n",
    "\n",
    "# for file in os.listdir(maindir):\n",
    "#     if file.endswith('.txt'):\n",
    "#         lang = os.path.splitext(os.path.basename(file))[0].split('w')[0]\n",
    "#         print(lang)\n",
    "\n",
    "for file in os.listdir(maindir):\n",
    "    if file.endswith('.txt'):\n",
    "        fullpath = os.path.join(maindir, file)\n",
    "        lang = os.path.splitext(os.path.basename(file))[0].split('w')[0]\n",
    "        # current_wc = 0\n",
    "        if lang in alphabets:\n",
    "            alpha = set(alphabets[lang])\n",
    "        with open(f'/Users/k/Docs/School/Tuebingen/Thesis/iscl-thesis/2024_corpora/{lang}_wiki_cleaned.txt', 'w', encoding='utf-8') as out:\n",
    "            with open(fullpath, 'r', encoding='utf-8') as f: \n",
    "                content = f.read()\n",
    "                f.close()\n",
    "            sentences = clean_sentences(content)\n",
    "            sen_count = 0\n",
    "            current_wc = 0\n",
    "            for sentence in sentences:\n",
    "                # Remove all chars not language's alphabet\n",
    "                s = ''.join(char if char in alpha or char.isspace() else '' for char in sentence)\n",
    "                # Remove leftover whitespaces\n",
    "                s = re.sub(r'\\s+', ' ', s).strip()\n",
    "                # Remove sentence fragments containing only 1 word\n",
    "                if len(s.split()) > 1:\n",
    "                    if (len(s.split()) + current_wc) < max_wc:\n",
    "                        current_wc += len(s.split())\n",
    "                        out.write(s + '\\n')\n",
    "                        sen_count += 1\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        print(f'{file_count}/24 {lang} corpus finished. {current_wc} words. {sen_count} sentences.')\n",
    "        file_count += 1\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
