{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, nltk, stopwordsiso, sklearn\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "# UD corpus\n",
    "with open('en_partut-ud-train.txt') as eng_f:\n",
    "    eng_text = eng_f.read().replace('\\n', '')\n",
    "    \n",
    "# Preprocessing\n",
    "def preprocess_english_text(text):\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Convert all characters to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Perform stemming or lemmatization\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Or\n",
    "    #lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Russian\n",
    "# UD corpus\n",
    "with open('ru_pud-ud-test.txt') as rus_f:\n",
    "    rus_text = rus_f.read().replace('\\n', '')\n",
    "    \n",
    "# Preprocessing\n",
    "def preprocess_russian_text(text):\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Convert all characters to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('russian')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Perform stemming or lemmatization\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Or\n",
    "    #lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arabic\n",
    "# UD corpus\n",
    "with open('ar_pud-ud-test.txt') as ar_f:\n",
    "    ar_text = ar_f.read().replace('\\n', '')\n",
    "    \n",
    "# Preprocessing\n",
    "def preprocess_arabic_text(text):\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Convert all characters to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Perform stemming or lemmatization\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Or\n",
    "    #lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mandarin Chinese\n",
    "# UD corpus\n",
    "with open('zh_pud-ud-test.txt') as zh_f:\n",
    "    zh_text = zh_f.read().replace('\\n', '')\n",
    "    \n",
    "# Preprocessing\n",
    "def preprocess_chinese_text(text):\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    # text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # text = re.sub(r'[^\\u4e00-\\u9fff]+', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    # tokens = nltk.tokenize.word_tokenize(text)\n",
    "    \n",
    "    # Tokenize with Jieba\n",
    "    tokens = \"|\".join(jieba.cut(zh_text, cut_all=False, HMM=True))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    # stopwords = stopwordsiso.stopwords(\"zh\")\n",
    "    # tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the CountVectorizer class\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# fit the vectorizer on the tokenized text data\n",
    "X = vectorizer.fit_transform(tokens)\n",
    "\n",
    "# get the feature names\n",
    "feature_names = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German\n",
    "# UD corpus\n",
    "with open('de_pud-ud-test.txt') as de_f:\n",
    "    de_text = de_f.read().replace('\\n', '')\n",
    "    \n",
    "# Preprocessing\n",
    "def preprocess_german_text(text):\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Convert all characters to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('german')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Perform stemming or lemmatization\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Or\n",
    "    #lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japanese\n",
    "# UD corpus\n",
    "with open('ja_pud-ud-test.txt') as ja_f:\n",
    "    ja_text = ja_f.read().replace('\\n', '')\n",
    "    \n",
    "# Preprocessing\n",
    "def preprocess_japanese_text(text):\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Convert all characters to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('japanese')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Perform stemming or lemmatization\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Or\n",
    "    #lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindi\n",
    "# UD corpus\n",
    "with open('hi_pud-ud-test.txt') as hi_f:\n",
    "    hi_text = hi_f.read().replace('\\n', '')\n",
    "    \n",
    "# Preprocessing\n",
    "def preprocess_hindi_text(text):\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Convert all characters to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords = stopwordsiso.stopwords('hi')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Perform stemming or lemmatization\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Or\n",
    "    #lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
